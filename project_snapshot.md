# ğŸ§  Project Snapshot

Tento soubor obsahuje strukturu projektu a obsah jednotlivÃ½ch souborÅ¯ pro pouÅ¾itÃ­ s AI asistenty.

## ğŸ“‚ Struktura projektu

```
UniversalAIAgent/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ agent
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py
â”‚   â””â”€â”€ core2.py
â”œâ”€â”€ clear_rag_memory.py
â”œâ”€â”€ cli
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ main2.py
â”‚   â”œâ”€â”€ ui.py
â”‚   â””â”€â”€ ui2.py
â”œâ”€â”€ config.json
â”œâ”€â”€ persistent_chat_history.json
â”œâ”€â”€ prompts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ epic.jinja2
â”‚   â”œâ”€â”€ idea.jinja2
â”‚   â””â”€â”€ user_stories.jinja2
â”œâ”€â”€ rag_chroma_db_v2
â”‚   â””â”€â”€ chroma.sqlite3
â”œâ”€â”€ rag_confluence_loader.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ input_loader.py
â”‚   â”œâ”€â”€ jira_content_service.py
â”‚   â”œâ”€â”€ jira_service.py
â”‚   â”œâ”€â”€ rag_service.py
â”‚   â””â”€â”€ web_service.py
â”œâ”€â”€ tools
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ input_loader.py
â”‚   â”œâ”€â”€ jira_content_tools.py
â”‚   â”œâ”€â”€ jira_tools.py
â”‚   â”œâ”€â”€ rag_tools.py
â”‚   â””â”€â”€ web_tools.py
â”œâ”€â”€ ui.py
â”œâ”€â”€ ui2.py
â””â”€â”€ utils
    â””â”€â”€ io_utils.py
```

## ğŸ“„ Obsahy souborÅ¯


---

### `.gitignore`

```python
# Python
__pycache__/
*.py[cod]
*.egg-info/
.env
.vscode/
.idea/
.DS_Store

# Virtual environments
venv/
.env/

# Logs
*.log

# Vector db
rag_chroma_db/

output/
```


---

### `clear_rag_memory.py`

```python
#!/usr/bin/env python3
"""
clear_rag_memory.py
===================

JednorÃ¡zovÃ© promazÃ¡nÃ­ a/nebo validace Chroma DB ('rag_chroma_db/') pouÅ¾Ã­vanÃ© jako RAG pamÄ›Å¥.

â€¢ SmaÅ¾e nebo zazÃ¡lohuje DB a ovÄ›Å™Ã­, Å¾e po operaci nezÅ¯staly Å¾Ã¡dnÃ© vektory.
â€¢ ZÃ¡vislost na 'chromadb' je volitelnÃ¡ â€“ bez nÃ­ probÄ›hne fallback validace pÅ™es velikost souborÅ¯.

MazÃ¡nÃ­ + automatickÃ¡ validace	
    python clear_rag_memory.py -y	
    Po smazÃ¡nÃ­ ovÄ›Å™Ã­, jestli je DB opravdu prÃ¡zdnÃ¡; pokud ne, skript skonÄÃ­ s Neo (1).

ZÃ¡loha + validace	
    python clear_rag_memory.py --backup -y	
    PÅ™esune DB do zÃ¡lohy, vytvoÅ™Ã­ ÄistÃ½ adresÃ¡Å™, zkontroluje.

PouhÃ¡ kontrola (napÅ™. v CI)	
    python clear_rag_memory.py --check	
    NemÄ›nÃ­ data, jen vrÃ¡tÃ­ 0/1 podle stavu (lze pouÅ¾Ã­t v bash if â€¦).

"""

from __future__ import annotations

import argparse
import datetime as _dt
import pathlib as _pl
import shutil as _sh
import sys as _sys
from typing import Optional

try:
    import chromadb
except ImportError:  # pragma: no cover
    chromadb = None  # type: ignore

ROOT = _pl.Path(__file__).resolve().parent
DB_DIR = ROOT / "rag_chroma_db"


# ------------------------------------------------------------------------------
# Utility: poÄet vektorÅ¯ v DB
# ------------------------------------------------------------------------------

def _count_vectors() -> int:
    """
    SeÄte celkovÃ½ poÄet embeddingÅ¯ ve vÅ¡ech kolekcÃ­ch.

    VracÃ­ 0, pokud:
      â€¢ DB adresÃ¡Å™ neexistuje
      â€¢ NenÃ­ dostupnÃ¡ knihovna chromadb a souÄasnÄ› je adresÃ¡Å™ prÃ¡zdnÃ½
    """
    if not DB_DIR.exists():
        return 0

    if chromadb is None:
        # Fallback: spoÄÃ­tÃ¡me soubory vÄ›tÅ¡Ã­ neÅ¾ 1 KiB (pravdÄ›p. data)
        return sum(1 for p in DB_DIR.rglob("*") if p.is_file() and p.stat().st_size > 1024)

    client = chromadb.PersistentClient(path=str(DB_DIR))
    total = 0
    for col_meta in client.list_collections():
        col = client.get_collection(col_meta.name)
        total += col.count()
    return total


def _validate_empty() -> bool:
    """VrÃ¡tÃ­ True, kdyÅ¾ vektorÅ¯ == 0."""
    leftover = _count_vectors()
    if leftover == 0:
        print("âœ… RAG pamÄ›Å¥ je prÃ¡zdnÃ¡, hotovo.")
        return True

    print(f"âŒ V pamÄ›ti zÅ¯stalo jeÅ¡tÄ› {leftover} vektorÅ¯!", file=_sys.stderr)
    return False


# ------------------------------------------------------------------------------
# HlavnÃ­ operace (wipe / backup)
# ------------------------------------------------------------------------------

def _wipe_rag_db(force: bool, backup: bool) -> None:
    """SmaÅ¾e nebo zÃ¡lohuje adresÃ¡Å™ rag_chroma_db/ a pak jej znovu vytvoÅ™Ã­."""
    if not DB_DIR.exists():
        print("AdresÃ¡Å™ 'rag_chroma_db' neexistuje â€“ RAG pamÄ›Å¥ je uÅ¾ prÃ¡zdnÃ¡.")
        return

    if not force:
        prompt = f"Toto {'zÃ¡lohuje a potÃ© ' if backup else ''}SMAÅ½E '{DB_DIR}'. PokraÄovat? [y/N] "
        if input(prompt).strip().lower() not in {"y", "yes"}:
            print("Operace zruÅ¡ena.")
            return

    if backup:
        stamp = _dt.datetime.now().strftime("%Y%m%d_%H%M%S")
        dest = DB_DIR.with_name(f"rag_chroma_db_backup_{stamp}")
        _sh.move(str(DB_DIR), dest)
        print(f"VektorovÃ¡ DB byla pÅ™esunuta do: {dest}")
    else:
        _sh.rmtree(DB_DIR)
        print("VektorovÃ¡ DB byla nenÃ¡vratnÄ› smazÃ¡na.")

    DB_DIR.mkdir(exist_ok=True)
    print("VytvoÅ™en novÃ½ prÃ¡zdnÃ½ 'rag_chroma_db'.")


# ------------------------------------------------------------------------------
# CLI
# ------------------------------------------------------------------------------

def main() -> None:  # noqa: D401 â€“ krÃ¡tkÃ½ nÃ¡zev je OK
    parser = argparse.ArgumentParser(
        description="VymaÅ¾e (nebo zazÃ¡lohuje) perzistentnÃ­ RAG pamÄ›Å¥ a ovÄ›Å™Ã­, Å¾e je prÃ¡zdnÃ¡."
    )
    parser.add_argument("-y", "--yes", action="store_true", help="Provede akci bez interaktivnÃ­ho potvrzenÃ­.")
    parser.add_argument("--backup", action="store_true", help="PÅ™ed smazÃ¡nÃ­m vytvoÅ™Ã­ timestampovanou zÃ¡lohu.")
    parser.add_argument("--check", action="store_true", help="Pouze zkontroluje, zda je pamÄ›Å¥ prÃ¡zdnÃ¡ (nic nemaÅ¾e).")
    args = parser.parse_args()

    if args.check:
        ok = _validate_empty()
        _sys.exit(0 if ok else 1)

    _wipe_rag_db(force=args.yes, backup=args.backup)
    ok = _validate_empty()
    _sys.exit(0 if ok else 1)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        _sys.exit("\nPÅ™eruÅ¡eno uÅ¾ivatelem.")

```


---

### `config.json`

```python
{
  "jira": {
    "url":  "https://productoo.atlassian.net/",
    "user": "tomas.ventruba@productoo.com",
    "jql":   "project = P4 ORDER BY created DESC",
    "jira_project_key": "P4",
    "maxResults": 50,
    "roadmap_url": "https://roadmap.productoo.com/p4/Working-version/"
  },
    "confluence": {
        "base_url": "https://productoo.atlassian.net",
        "user": "tomas.ventruba@productoo.com",
        "spaces": {
            "documentation": "PD",
            "roadmap": "PD"
        },
        "ancestor_ids": [
            "677183489",
            "99745795"
        ]
  }
}
```


---

### `persistent_chat_history.json`

```python
[]
```


---

### `rag_confluence_loader.py`

```python
#!/usr/bin/env python3
"""
rag_confluence_loader.py

Load Confluence pages (and their descendants) into a Chroma vectorstore for RAG.
Skips pages already present in the vectorstore by page_id metadata.
"""

import os
import json
from pathlib import Path

from dotenv import load_dotenv

# â”€â”€ Suppress Chroma telemetry errors â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Must be set before importing Chroma
os.environ["CHROMA_TELEMETRY"] = "0"

from langchain_community.document_loaders import ConfluenceLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter


def load_config(config_path: Path) -> dict:
    with open(config_path, "r", encoding="utf-8") as f:
        return json.load(f)


def get_existing_page_ids(vectorstore: Chroma) -> set:
    """Retrieve page_id metadata from all documents currently in the vectorstore."""
    col = vectorstore._collection
    metadatas = col.get(include=["metadatas"])["metadatas"]
    return { str(m.get("page_id") or m.get("id")) for m in metadatas if m.get("page_id") or m.get("id") }


def main():
    # â”€â”€ Load environment and config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    load_dotenv()
    token = os.getenv("JIRA_AUTH_TOKEN")
    if not token:
        raise RuntimeError("Missing JIRA_AUTH_TOKEN in environment")

    config_path = Path(__file__).parent / "config.json"
    cfg = load_config(config_path)
    conf_cfg = cfg.get("confluence", {})
    base_url = conf_cfg["base_url"]
    user = conf_cfg["user"]
    ancestor_ids = conf_cfg.get("ancestor_ids", [])
    if not ancestor_ids:
        print("No ancestor_ids configured. Nothing to load.")
        return

    # â”€â”€ Prepare embeddings & vectorstore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma(
        persist_directory="rag_chroma_db",
        embedding_function=embeddings
    )

    existing_ids = get_existing_page_ids(vectorstore)
    print(f"{len(existing_ids)} pages already in vectorstore.")

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    new_chunks = []

    # â”€â”€ Load & filter Confluence pages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for anc in ancestor_ids:
        print(f"â†’ Fetching descendants of Confluence page {anc} â€¦")
        loader = ConfluenceLoader(
            url=base_url,
            username=user,
            api_key=token,
            cql=f"(id={anc} OR ancestor={anc})",
            include_archived_content=False,
            include_restricted_content=False
        )
        docs = loader.load()
        print(f"   â€¢ Retrieved {len(docs)} pages under ancestor {anc}")

        fresh = []
        for doc in docs:
            pid = str(doc.metadata.get("id") or doc.metadata.get("page_id"))
            if pid in existing_ids:
                print(f"     â€“ Skipping already indexed page {pid}")
                continue
            doc.metadata["page_id"] = pid
            fresh.append(doc)

        if not fresh:
            continue

        chunks = splitter.split_documents(fresh)
        print(f"Chunked into {len(chunks)} pieces")
        new_chunks.extend(chunks)

    # â”€â”€ Add only new chunks to Chroma â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if new_chunks:
        vectorstore.add_documents(new_chunks)
        print(f"Added {len(new_chunks)} new document chunks.")
    else:
        print("No new documents to add.")

if __name__ == "__main__":
    main()

```


---

### `README.md`

```python
# ğŸ§  Universal AI Agent (ProductooÂ P4)

*Conversational assistant for product managers, analysts and engineers working on Productooâ€™s P4 manufacturing suite.*

---

## âœ¨ What it does now

| Category                      | Status          | Details                                                                                                                 |
| ----------------------------- | --------------- | ----------------------------------------------------------------------------------------------------------------------- |
| **Conversational interface**  | **âœ…**           | CLI (`main.py`) **and** lightweight Gradio UI (`ui.py`).                                                                |
| **Knowledge retrieval (RAG)** | **âœ…**           | Chroma vectorâ€‘store (`rag_chroma_db/`) continuously enriched with every Q\&A turn.                                      |
| **Web search**                | **âœ…**           | DuckDuckGo (`searchWeb`) & Wikipedia snippet tool.                                                                      |
| **Semantic web search**       | **Î²**           | Tavily semantic search if `TAVILY_API_KEY` is present.                                                                  |
| **Jira integration**          | **âœ…**           | `jira_ideas_retriever` â€“ lists *Idea* issues matching an optional keyword.                                              |
| **File output**               | **âœ…**           | `save_text_to_file` stores each answer in a *new* timestamped file under `./output/`. Visible & downloadable in the UI. |
| **Confluence loader**         | **âœ… (offline)** | `rag_confluence_loader.py` indexes Confluence pages into RAG (manual run).                                              |
| **Continuous learning**       | **â†º**           | Every chat exchange is appended to the vector store for longâ€‘term memory.                                               |

---

## ğŸ” Available tools

| Tool name              | Purpose                                                                                   |
| ---------------------- | ----------------------------------------------------------------------------------------- |
| `searchWeb`            | Quick DuckDuckGo search (definitions, news, blogs).                                       |
| `wikipedia_query`      | Short summary from Wikipedia.                                                             |
| `rag_retriever`        | Fetch up toÂ 4 most relevant chunks from the internal vector store (docs, roadmap, chats). |
| `jira_ideas_retriever` | List *Ideas* from Jira project **P4**; optional `keyword` filter.                         |
| `tavily_search`        | LLMâ€‘powered semantic web search (requires `TAVILY_API_KEY`).                              |
| `save_text_to_file`    | Persist any text to `output/â€¦` (timestamped).                                             |

> **PlannedÂ tool** â€“ `jira_issue_detail`: fetch a **single** Jira issue by key (e.g.Â `P4â€‘1234`) with full description, acceptance criteria, subtasks & comments.
> *Benefit:* quick deepâ€‘dives, faster duplicate detection.

---

## ğŸ—Â Architecture overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     user query / feedback
â”‚   Gradio UI   â”‚  â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
       â”‚ HTTP                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”     internal call        â”‚
â”‚  LangChain    â”‚  â”€â”€â–¶  Tool Router  â”€â”€â–¶â”€â”€â”€â”˜
â”‚   Agent       â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â–¼
   â”‚     â”‚    â”‚    structured/tool calls
   â”‚     â”‚    â”‚
   â”‚     â”‚    â””â”€â–¶ Jira API (ideas / issueâ€‘detail)
   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Web search (DuckDuckGo / Tavily)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ RAGÂ vector store (Chroma)
```

---

## ğŸš€ Quick start

```bash
# 1. Install deps (create venv beforehand)
pip install -r requirements.txt

# 2. Configure API keys (.env)
cp .env.example .env  # fill in OPENAI_API_KEY, JIRA_AUTH_TOKEN â€¦

# 3a. Run conversational CLI
python main.py

# 3b. Or launch local web UI
python ui.py  # opens http://127.0.0.1:7860
```

### Required environment variables

```dotenv
OPENAI_API_KEY="skâ€‘â€¦"
JIRA_AUTH_TOKEN="atlassianâ€‘â€¦"
TAVILY_API_KEY=""           # optional
```

---

## ğŸ—‚Â Project layout

```
ğŸ“ universalagent/
â”œâ”€â”€ main.py                # conversational loop
â”œâ”€â”€ tools.py               # LangChain Tools (search, Jira, save, â€¦)
â”œâ”€â”€ ui.py                  # Gradio frontâ€‘end
â”œâ”€â”€ jira_retriever.py      # lowâ€‘level Jira REST helper
â”œâ”€â”€ rag_confluence_loader.py  # import Confluence pages into RAG
â”œâ”€â”€ rag_vectorstore.py     # bulkâ€‘import local docs into RAG
â”œâ”€â”€ output/                # timestamped txt exports (gitâ€‘ignored)
â””â”€â”€ rag_chroma_db/         # vector DB (gitâ€‘ignored)
```

---

## ğŸ›£Â NextÂ steps (roadmap)

| Priority | Item                                     | Rationale                                                                               |
| -------- | ---------------------------------------- | --------------------------------------------------------------------------------------- |
| **â¬†**    | **`jira_issue_detail` Tool**             | Fetch full Jira issue by key; enable deep context for duplicates & acceptance criteria. |                                            |
| Â â€”Â       | Confluence incremental sync              | Schedule nightly run; mark removed pages as archived in RAG.                            |
| Â â€”Â       | Autoâ€‘summarise fresh Jira tickets to RAG | â€œChronicleâ€ new issues daily for fast retrieval.                                        |
| Â â€”Â       | Duplicateâ€‘idea detector                  | Hash & embedding similarity across JiraÂ Ideas.                                          |
| Â â€”Â       | KPI dashboard                            | Track solved tickets, average cycleâ€‘time, top requested features.                       |                                |
| Â â€”Â       | Unit & integration tests                 | pytest + Playwright for UI workflows.                                                   |
| Â â€”Â       | Create Jira Epics, Stories & Release notes  | Write content of jira issues                                                  |

Contributions & ideas welcome â€“ open anÂ issue or ping **@tomas.ventruba**.

```


---

### `requirements.txt`

```python
langchain>=0.2.9,<1.0
wikipedia
langchain-community
langchain-openai
langchain-anthropic
langchain-chroma
langchain-unstructured
python-dotenv
pydantic
duckduckgo-search
tavily-python
atlassian-python-api
gradio
pytest
chromadb
unstructured
jinja2
langgraph
```


---

### `ui.py`

```python
"""Compatibility wrapper for the Gradio UI entry point."""
from cli.ui import launch

if __name__ == "__main__":
    launch()

```


---

### `ui2.py`

```python
"""Entryâ€‘point pro Gradio UI nad core2."""
from cli.ui2 import launch

if __name__ == "__main__":
    launch()

```


---

### `agent\core.py`

```python
# agent/core.py
from __future__ import annotations

import os
from datetime import datetime
from pathlib import Path

from dotenv import load_dotenv
import openai
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.memory.buffer import ConversationBufferMemory
from langchain.schema import Document
from langchain_chroma import Chroma
from chromadb.config import Settings
from pydantic import BaseModel

from tools import (
    search_tool,
    wiki_tool,
    save_tool,
    rag_tool,
    tavily_tool,
    jira_ideas,
    jira_issue_detail,
    jira_duplicates,
    jira_content_tools,
    process_input_tool,
    jira_update_description,
    jira_child_issues,
    jira_issue_links,
)

# ---------------------------------------------------------------------------
# Environment & telemetry
# ---------------------------------------------------------------------------
load_dotenv()

os.environ.setdefault("OPENAI_TELEMETRY", "0")
if hasattr(openai, "telemetry") and hasattr(openai.telemetry, "TelemetryClient"):
    openai.telemetry.TelemetryClient.capture = lambda *_, **__: None



# ---------------------------------------------------------------------------
# Vectorstore (RAG) setup
# ---------------------------------------------------------------------------
CHROMA_DIR = "rag_chroma_db"
_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
_vectorstore = Chroma(persist_directory=CHROMA_DIR, embedding_function=_embeddings)


def _store_exchange(question: str, answer: str) -> None:
    """Save the dialogue pair into the RAG vector store for future retrieval."""
    ts = datetime.utcnow().isoformat()
    docs = [
        Document(page_content=f"USER: {question}", metadata={"role": "user", "ts": ts}),
        Document(page_content=f"ASSISTANT: {answer}", metadata={"role": "assistant", "ts": ts}),
    ]
    _vectorstore.add_documents(docs)


# ---------------------------------------------------------------------------
# Structured output schema
# ---------------------------------------------------------------------------
class ResearchResponse(BaseModel):
    topic: str
    summary: str
    sources: list[str]
    tools_used: list[str]


parser = PydanticOutputParser(pydantic_object=ResearchResponse)

# ---------------------------------------------------------------------------
# Core prompt template
# ---------------------------------------------------------------------------
system_prompt = (
    """
You are **Productooâ€™s senior AI assistant** specialised in manufacturing software P4.
Your mission:
  1. Analyse market trends, the current P4 roadmap and system state.
  2. Propose concrete next steps maximising customer value and minimising techâ€‘debt.
  3. Seamlessly use the available tools (`searchWeb`, `rag_retriever`,
     `tavily_search`, `jira_ideas_retriever`, `save_text_to_file`) when relevant
     and clearly cite your sources.

Guidelines:
- Think stepâ€‘byâ€‘step, reason explicitly yet concisely.
- Ask clarifying questions whenever the userâ€™s request is ambiguous.
- Make answers informationâ€‘denseâ€”avoid filler.
- Prefer actionable recommendations backed by evidence and quantified impact.
- **After every answer, end with exactly:** _"JeÅ¡tÄ› nÄ›co, s ÄÃ­m mohu pomoci?"_
  (keeps the dialogue open).

Return your answer strictly as valid JSON conforming to the schema below.
{format_instructions}
"""
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("placeholder", "{chat_history}"),
        ("human", "{query}"),
        ("placeholder", "{agent_scratchpad}"),
    ]
).partial(format_instructions=parser.get_format_instructions())

# ---------------------------------------------------------------------------
# LLM & conversation memory
# ---------------------------------------------------------------------------
_llm = ChatOpenAI(model="gpt-4o", temperature=0.3)
_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ---------------------------------------------------------------------------
# Agent assembly
# ---------------------------------------------------------------------------
_TOOLS = [
    search_tool,
    wiki_tool,
    save_tool,
    rag_tool,
    tavily_tool,
    jira_ideas,
    jira_issue_detail,
    jira_duplicates,
    *jira_content_tools,
    process_input_tool,
    jira_update_description,
    jira_child_issues,
    jira_issue_links,
]
_agent = create_tool_calling_agent(llm=_llm, prompt=prompt, tools=_TOOLS)
agent_executor = AgentExecutor(
    agent=_agent,
    tools=_TOOLS,
    memory=_memory,
    verbose=True,
    return_intermediate_steps=True,
    )


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------
def handle_query(query: str) -> str:
    """VrÃ¡tÃ­ plnÃ½ nÃ¡vrh (Markdown), pokud je k dispozici, jinak fallback na summary."""
    raw = agent_executor.invoke({"query": query})
    full = next(
        (obs for _act, obs in reversed(raw.get("intermediate_steps", []))
         if isinstance(obs, str) and obs.strip()),
        "",
    )
    answer = full or raw.get("output", "")
    _store_exchange(query, answer)
    return answer


__all__ = ["handle_query", "agent_executor", "ResearchResponse", "parser"]

```


---

### `agent\core2.py`

```python
# agent/core2.py
"""
LangGraphâ€‘based AI core for Productoo P4 agent.
Keeps the same public API as core.py (handle_query) so both can coexist.

Key features
------------
â€¢ Multiâ€‘tier memory (shortâ€‘term window, persistent file log, longâ€‘term vector store)
â€¢ Continuous learning (= automatic RAG enrichment after each exchange)
â€¢ Full toolâ€‘calling capability identical to core.py
"""

from __future__ import annotations

import os
from datetime import datetime
from typing import TypedDict, Any, List

from dotenv import load_dotenv
import openai

# LangChainÂ &Â LangGraph
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.memory import ConversationBufferWindowMemory
from langchain_community.chat_message_histories import FileChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import Document
from langchain.agents import create_tool_calling_agent, AgentExecutor

from langgraph.graph import StateGraph, END

# Vector store
from langchain_chroma import Chroma
from chromadb.config import Settings

# Projectâ€‘specific tools (identickÃ© sÂ core.py)
from tools import (
    search_tool,
    wiki_tool,
    save_tool,
    rag_tool,
    tavily_tool,
    jira_ideas,
    jira_issue_detail,
    jira_duplicates,
    jira_content_tools,
    process_input_tool,
    jira_update_description,
    jira_child_issues,
    jira_issue_links,
)

# ---------------------------------------------------------------------------
# Environment & telemetry (be consistent with core.py)
# ---------------------------------------------------------------------------
load_dotenv()

os.environ.setdefault("OPENAI_TELEMETRY", "0")
os.environ.setdefault("CHROMA_TELEMETRY",  "0")
if hasattr(openai, "telemetry") and hasattr(openai.telemetry, "TelemetryClient"):
    openai.telemetry.TelemetryClient.capture = staticmethod(lambda *a, **k: None)

# ---------------------------------------------------------------------------
# Vectorstore (shared longâ€‘term memory)
# ---------------------------------------------------------------------------
CHROMA_DIR = os.getenv("CHROMA_DIR_V2", "rag_chroma_db")
_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
_vectorstore = Chroma(
    persist_directory=CHROMA_DIR,
    embedding_function=_embeddings,
    client_settings=Settings(anonymized_telemetry=False),
)

# ---------------------------------------------------------------------------
# Multiâ€‘tier memory configuration
# ---------------------------------------------------------------------------
SHORT_WINDOW = int(os.getenv("AGENT_SHORT_WINDOW", 10))

# 1) Shortâ€‘term window (keeps last N turns in RAM)
_short_term_memory = ConversationBufferWindowMemory(
    k=SHORT_WINDOW,
    memory_key="chat_history",
    return_messages=True,
)

# 2) Persistent chat log (restored across restarts â†’ feeds the window buffer)
_persistent_history_file = os.getenv(
    "PERSISTENT_HISTORY_FILE",
    "persistent_chat_history.json"
    )
_short_term_memory.chat_memory = FileChatMessageHistory(file_path=_persistent_history_file)

# ---------------------------------------------------------------------------
# Prompt, LLM a agent stejnÄ› jako dÅ™Ã­ve
# ---------------------------------------------------------------------------
system_prompt = """
You are **Productooâ€™s senior AI assistant** specialised in manufacturing software P4.
Follow the guidelines from the original core while using available tools.
Always finish with: *JeÅ¡tÄ› nÄ›co, s ÄÃ­m mohu pomoci?*
""".strip()

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{query}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ]
)

_llm = ChatOpenAI(model="gpt-4o", temperature=0.3)

_TOOLS = [
    search_tool,
    wiki_tool,
    save_tool,
    rag_tool,
    tavily_tool,
    jira_ideas,
    jira_issue_detail,
    jira_duplicates,
    *jira_content_tools,
    process_input_tool,
    jira_update_description,
    jira_child_issues,
    jira_issue_links,
]

_agent = create_tool_calling_agent(llm=_llm, prompt=prompt, tools=_TOOLS)
_agent_executor = AgentExecutor(
    agent=_agent,
    tools=_TOOLS,
    memory=_short_term_memory,
    verbose=True,
    return_intermediate_steps=True,
)

# ---------------------------------------------------------------------------
# LangGraph: state model & nodes
# ---------------------------------------------------------------------------
class AgentState(TypedDict):
    """DatovÃ½ balÃ­Äek pÅ™enÃ¡Å¡enÃ½ mezi uzly grafu."""
    query: str
    answer: str
    intermediate_steps: list
    retrieved_context: str

# --- Node 1: Retrieve relevant longâ€‘term memory --------------------------------
def recall(state: AgentState) -> AgentState:
    """VyhledÃ¡ vektorovÄ› relevantnÃ­ minulou konverzaci / znalosti."""
    docs = _vectorstore.similarity_search(state["query"], k=4)
    state["retrieved_context"] = "\n".join(d.page_content for d in docs)
    return state

# --- Node 2: Call the langchain agent -----------------------------------------
def call_agent(state: AgentState) -> AgentState:
    """SpustÃ­ nÃ¡strojâ€‘volajÃ­cÃ­ho agenta sÂ krÃ¡tkodobou pamÄ›tÃ­ + kontextem."""
    q = state["query"]
    ctx = state.get("retrieved_context", "")
    if ctx:
        q = f"{q}\n\nRelevant past context:\n{ctx}"
    result = _agent_executor.invoke({"query": q})
    state["answer"] = result["output"]
    state["intermediate_steps"] = result["intermediate_steps"]
    return state

# --- Node 3: Learn (append to vectorstore) ------------------------------------
def learn(state: AgentState) -> AgentState:
    """ZapÃ­Å¡e dialog do dlouhodobÃ© pamÄ›ti Po kaÅ¾dÃ©m bÄ›hu."""
    ts = datetime.utcnow().isoformat()
    _vectorstore.add_documents(
        [
            Document(page_content=f"USER: {state['query']}", metadata={"role": "user", "ts": ts}),
            Document(page_content=f"ASSISTANT: {state['answer']}", metadata={"role": "assistant", "ts": ts}),
        ]
    )
    return state

# ---------------------------------------------------------------------------
# Graf a workflow
# ---------------------------------------------------------------------------
graph = StateGraph(input_type=AgentState)
graph.add_node("recall", recall)
graph.add_node("agent", call_agent)
graph.add_node("learn", learn)

graph.set_entry_point("recall")
graph.add_edge("recall", "agent")
graph.add_edge("agent", "learn")
graph.add_edge("learn", END)

# KompilovanÃ½ workflow (lazyâ€‘initialised, aby import nezdrÅ¾oval start)
workflow = graph.compile()

# ---------------------------------------------------------------------------
# VeÅ™ejnÃ© API â€“ zÅ¯stÃ¡vÃ¡ stejnÃ© jako v core.py
# ---------------------------------------------------------------------------
def handle_query(query: str) -> str:
    """JedinÃ½ veÅ™ejnÃ½ vstup: zpracuje dotaz a vrÃ¡tÃ­ odpovÄ›Ä."""
    init_state: AgentState = {"query": query, "answer": "", "intermediate_steps": [], "retrieved_context": ""}
    final_state = workflow.invoke(init_state)
    return final_state["answer"]  # + krÃ¡tkÃ© 'nÄ›co dalÅ¡Ã­ho' je uÅ¾ v promptu

# Convenience alias pro pÅ™Ã­padnÃ© externÃ­ diagnostiky
agent_workflow = workflow

__all__ = ["handle_query", "agent_workflow"]

```


---

### `agent\__init__.py`

```python
# agent/__init__.py
from importlib import import_module
import os

_selected = os.getenv("AGENT_VERSION", "v1").lower()   # v1 | v2
if _selected == "v2":
    core2 = import_module(".core2", __name__)
    handle_query = core2.handle_query
    agent_workflow = core2.agent_workflow
else:
    core = import_module(".core", __name__)
    handle_query = core.handle_query
    agent_executor = core.agent_executor
    ResearchResponse = core.ResearchResponse
    parser = core.parser

__all__ = [
    "handle_query",
    "agent_executor",
    "agent_workflow",
    "ResearchResponse",
    "parser",
]

```


---

### `cli\main.py`

```python
# cli/main.py
from __future__ import annotations

from agent import handle_query, parser, ResearchResponse


def main() -> None:
    """Launch interactive CLI until user exits."""
    print("\nUniversal AI Agent (Productoo) â€” napiÅ¡te 'exit' pro ukonÄenÃ­.\n")
    while True:
        try:
            user_query = input("Vy: ").strip()
            if user_query.lower() in {"exit", "quit", "bye"}:
                print("Asistent: RÃ¡d jsem pomohl! MÄ›jte se.")
                break

            answer = handle_query(user_query)

            try:
                structured: ResearchResponse = parser.parse(answer)
                print(f"\nAsistent: {structured.summary}\n")
                if structured.sources:
                    print("Zdroj(e):", ", ".join(structured.sources))
            except Exception:
                print("\nAsistent:", answer)

        except KeyboardInterrupt:
            print("\nAsistent: KonÄÃ­m. MÄ›jte se.")
            break


if __name__ == "__main__":
    main()

```


---

### `cli\main2.py`

```python
# cli/main2.py
from __future__ import annotations
from agent.core2 import handle_query   # â† dÅ¯leÅ¾itÃ©

BANNER = "\nUniversal AI Agent â€¢ *core2* (LangGraph) â€” napiÅ¡te 'exit' pro ukonÄenÃ­.\n"

def main() -> None:
    print(BANNER)
    while True:
        try:
            q = input("Vy: ").strip()
            if q.lower() in {"exit", "quit", "bye"}:
                print("Asistent: RÃ¡d jsem pomohl! MÄ›jte se.")
                break
            print("\nAsistent:", handle_query(q), "\n")
        except KeyboardInterrupt:
            print("\nAsistent: KonÄÃ­m. MÄ›jte se.")
            break

if __name__ == "__main__":
    main()

```


---

### `cli\ui.py`

```python
# cli/ui.py
"""Lightweight local front-end for the Universal AI Agent."""
from __future__ import annotations

import asyncio
import json
import os
import pathlib
import re
import warnings

import gradio as gr

from agent import handle_query

warnings.filterwarnings("ignore", message=".*LangChainDeprecationWarning.*")
os.environ.setdefault("GRADIO_ANALYTICS_ENABLED", "false")

OUTPUT_DIR = pathlib.Path("output")
OUTPUT_DIR.mkdir(exist_ok=True)


def list_files() -> list[str]:
    return sorted(p.name for p in OUTPUT_DIR.iterdir() if p.is_file())


def read_file(fname: str) -> str:
    path = OUTPUT_DIR / fname
    try:
        return path.read_text("utf-8")
    except (UnicodeDecodeError, FileNotFoundError):
        return f"[Nelze zobrazit: binÃ¡rnÃ­ nebo neexistuje] {fname}"


def file_path(fname: str) -> str | None:
    p = OUTPUT_DIR / fname
    return str(p) if p.exists() else None


def refresh_choices():
    try:
        return gr.Dropdown.update(choices=list_files())
    except AttributeError:
        return gr.update(choices=list_files())


def pretty(raw: str) -> str:
    clean = raw.strip().removeprefix("```json").removesuffix("```").strip("` \n")
    m = re.search(r"\{.*\}", clean, re.DOTALL)
    if not m:
        return raw
    try:
        data = json.loads(m.group(0))
    except json.JSONDecodeError:
        return raw
    summary = data.get("summary", raw).strip()
    extras = []
    if src := data.get("sources"):
        extras.append("_**Zdroje:**_ " + ", ".join(map(str, src)))
    if tools := data.get("tools_used"):
        extras.append("_**NÃ¡stroje:**_ " + ", ".join(map(str, tools)))
    return summary + ("\n\n" + "\n".join(extras) if extras else "")


async def chat_fn(msg, history):
    history = history or []
    history.append({"role": "user", "content": msg})

    loop = asyncio.get_event_loop()
    raw = await loop.run_in_executor(None, lambda: handle_query(msg))
    history.append({"role": "assistant", "content": pretty(raw)})
    return history, history


def file_selected(fname):
    path = file_path(fname)
    preview = read_file(fname)
    try:
        file_update = gr.File.update(value=path, visible=bool(path))
    except AttributeError:
        file_update = gr.update(value=path, visible=bool(path))
    return preview, file_update


def trigger_download(fname):
    path = file_path(fname)
    try:
        return gr.File.update(value=path, visible=bool(path))
    except AttributeError:
        return gr.update(value=path, visible=bool(path))


def launch() -> None:
    with gr.Blocks(title="Universal AI Agent") as demo:
        gr.Markdown("## ğŸ’¬ AI Agent â€¢ ğŸ—‚ Output soubory")

        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(type="messages", height=420)
                msg = gr.Textbox(lines=2, placeholder="Zadej dotazâ€¦ (Ctrl+Enter)")
                msg.submit(chat_fn, [msg, chatbot], [chatbot, chatbot]).then(lambda: "", None, msg)

            with gr.Column():
                files = gr.Dropdown(choices=list_files(), label="Output soubory", interactive=True)
                content = gr.Textbox(label="NÃ¡hled obsahu", lines=14, interactive=False, show_copy_button=True)
                download_file = gr.File(label="Klikni pro staÅ¾enÃ­", visible=False)

                files.change(file_selected, files, [content, download_file])

                with gr.Row():
                    gr.Button("â†» Refresh").click(refresh_choices, None, files).then(
                        file_selected, files, [content, download_file]
                    )
                    gr.Button("â¬‡ StÃ¡hnout").click(trigger_download, files, download_file)

        if list_files():
            demo.load(file_selected, inputs=files, outputs=[content, download_file])

        demo.queue()
        demo.launch()


if __name__ == "__main__":
    launch()

```


---

### `cli\ui2.py`

```python
# cli/ui2.py
from __future__ import annotations
import asyncio, gradio as gr
from agent.core2 import handle_query   # â† dÅ¯leÅ¾itÃ©

async def chat_fn(msg, history):
    history = history or []
    history.append({"role": "user", "content": msg})
    loop = asyncio.get_event_loop()
    answer = await loop.run_in_executor(None, lambda: handle_query(msg))
    history.append({"role": "assistant", "content": answer})
    return history, history

def launch():
    with gr.Blocks(title="Universal AI Agent â€¢ core2") as demo:
        chatbot = gr.Chatbot(type="messages", height=420)
        msg = gr.Textbox(lines=2, placeholder="Zadej dotazâ€¦ (Ctrl+Enter)")
        msg.submit(chat_fn, [msg, chatbot], [chatbot, chatbot]).then(lambda: "", None, msg)
        demo.queue().launch()

if __name__ == "__main__":
    launch()

```


---

### `prompts\epic.jinja2`

```python
{# Jira Epic â€“ Czech #}
Jsi produktovÃ½ vlastnÃ­k a pÅ™ipravujeÅ¡ **Jiraâ€¯Epic** vâ€¯ÄeÅ¡tinÄ›.
Text je vÄ›cnÃ½, bez marketingovÃ½ch superlativÅ¯.

VraÅ¥ Markdown vÂ nÃ¡sledujÃ­cÃ­m poÅ™adÃ­:

**EpicÂ Goal** â€“ jedna vÄ›ta  
**Context** â€“ 1â€‘2 odstavce, propojuje problÃ©m a Å™eÅ¡enÃ­  
**Definition of Done** â€“ odrÃ¡Å¾ky  
**Acceptance criteria** â€“ Given / When / Then (â‰¤â€¯7)  
**OutÂ ofÂ scope** â€“ odrÃ¡Å¾ky

Idea
----
NÃ¡zev: {{ summary }}
Popis: {{ description or '(none)' }}

```


---

### `prompts\idea.jinja2`

```python
{# Jira Idea â€“ English, businessâ€‘ready #}
{%- set audience_phrase = {"business":"executives and sales",
                           "technical":"engineering teams",
                           "mixed":"crossâ€‘functional stakeholders"}[audience] %}
You are a senior product manager writing **concise, boardâ€‘ready Jira Ideas**.
Audience: {{ audience_phrase }}.
Tone: plain British English.
Limit total length to â‰¤ {{ max_words }} words.
Highlight the core insight in the first sentence.
Use exactly the headings you see in the example.

## Problem
Our maintenance costs grew 15â€¯% YoY due to frequent unplanned packagingâ€‘line stops.

## Proposed solution
Monitor vibration and temperature, predict failures 48â€¯h ahead and autoâ€‘schedule maintenance.

## Business value
- Reduce downtime by 10â€¯hâ€¯/â€¯month  
- Saveâ€¯â‚¬30â€¯kâ€¯per quarter  

## Acceptance criteria
- **Given** line sensors are online  
- **When** failure probabilityâ€¯>â€¯70â€¯% is detected  
- **Then** a work order is created and a maintenance slot reserved  

### Raw Idea
SUMMARY: {{ summary }}
DESCRIPTION: {{ description or '(none)' }}

### Task
Rewrite the Raw Idea into a polished Jira Idea description using the **same four headings** (Problem, Proposed solution, Business value, Acceptance criteria).  
Use active voice, quantify benefits, avoid jargon, keep total â‰¤ {{ max_words }} words.

```


---

### `prompts\user_stories.jinja2`

```python
{# INVEST User Stories â€“ Czech #}
Jsi agile coach. VytvoÅ™ **{{ count }} nezÃ¡vislÃ½ch user stories** zâ€¯tohotoâ€¯Epicu
vâ€¯ÄeÅ¡tinÄ› a dodrÅ¾ INVEST.

EPIC: {{ epic_name }}

{{ epic_description }}

FormÃ¡t kaÅ¾dÃ© story:

### <NÃ¡zev>
**User story**: Jako <persona> chci â€¦ aby â€¦  
**Acceptance criteria**
- Given / When / Then odrÃ¡Å¾ky  
**Estimate**: <S / M / L>

```


---

### `prompts\__init__.py`

```python

```


---

### `rag_chroma_db_v2\chroma.sqlite3`

```python


---

### `rag_chroma_db_v2\chroma.sqlite3`

```python
# Nelze naÄÃ­st soubor: 'utf-8' codec can't decode byte 0xfb in position 106: invalid start byte
```


---

### `services\input_loader.py`

```python
# services/input_loader.py

"""
Input Loader â€“ import files from ./input do Chroma vektorovÃ© DB
---------------------------------------------------------------
â¤ PÅ™etahuje soubory z lokÃ¡lnÃ­ sloÅ¾ky `./input` (pÅ™Ã­padnÄ› podmnoÅ¾inu podle
  argumentu) a indexuje je pro RAG dotazy.

Argumenty (string):
    ""            â†’ plnÃ½ sken (pÅ™eskoÄÃ­ uÅ¾ zaindexovanÃ© soubory)
    "force"       â†’ znovu zaindexuje vÅ¡echno (ignoruje fingerprinty)
    "pdf" / ".pdf"â†’ jen danou pÅ™Ã­ponu
    "subdir/foo"  â†’ jen danÃ½ (pod)adresÃ¡Å™
    "file.ext"    â†’ jen konkrÃ©tnÃ­ soubor

VracÃ­:
    Markdown report â€“ jedna Å™Ã¡dka na kaÅ¾dÃ½ novÄ› naimportovanÃ½ soubor.
"""
from __future__ import annotations

import hashlib
import inspect
from pathlib import Path
from typing import List

from langchain_chroma import Chroma
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores.utils import filter_complex_metadata

# ---- konfigurace -----------------------------------------------------------
CHROMA_DIR = "rag_chroma_db"
INPUT_DIR = Path("input")
INPUT_DIR.mkdir(exist_ok=True)

_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
_vectorstore = Chroma(
    collection_name="rag_store",
    embedding_function=_embeddings,
    persist_directory=CHROMA_DIR,
)

# ---- utilitky --------------------------------------------------------------
def _file_id(path: Path) -> str:
    """Fingerprint souboru = cesta::md5(content)."""
    h = hashlib.md5(path.read_bytes()).hexdigest()  # noqa: S324 â€“ lokÃ¡lnÃ­ hash
    return f"{path.as_posix()}::{h}"


def _already_indexed(file_id: str) -> bool:
    col = _vectorstore._collection
    return file_id in (m.get("file_id") for m in col.get(include=["metadatas"])["metadatas"])


def _summarise(text: str, llm: ChatOpenAI) -> str:
    prompt = "ShrÅˆ nÃ¡sledujÃ­cÃ­ dokument do max 1 vÄ›ty (ÄeÅ¡tina):\n" + text[:4000]
    return llm.invoke(prompt).content.strip()


# ---- robustnÃ­ sanitizace metadat ------------------------------------------
# API detekujeme dynamicky â€“ pokud selÅ¾e volÃ¡nÃ­ s Documentem, zkusÃ­me dict
def _sanitize_metadata(doc: Document) -> None:
    """ZaruÄÃ­, Å¾e v metadata zÅ¯stanou jen skalÃ¡ry a zavolÃ¡ korektnÃ­ variantu
    filter_complex_metadata bez ohledu na verzi knihovny."""
    try:
        # ğŸ…°ï¸ novÄ›jÅ¡Ã­ API (bere Document, mutuje in-place)
        filter_complex_metadata(doc)
    except Exception:
        try:
            # ğŸ…±ï¸ starÅ¡Ã­ API (bere dict, vracÃ­ dict)
            doc.metadata = filter_complex_metadata(doc.metadata)  # type: ignore[arg-type]
        except Exception:
            # ğŸ†˜ poslednÃ­ zÃ¡chrana â€“ ruÄnÄ› pÅ™evÃ©st neskalÃ¡rnÃ­ hodnoty na str
            doc.metadata = {
                k: (str(v) if isinstance(v, (list, dict, set, tuple)) else v)
                for k, v in doc.metadata.items()
            }


# ----------------------------------------------------------------------------
def process_input_files(arg: str = "") -> str:
    """HlavnÃ­ API volanÃ© LangChain toolem."""
    arg = arg.strip()
    force_reindex = arg.lower() == "force"

    # -- vyhodnocenÃ­ filtru ---------------------------------------------------
    ext_filter = None
    single_target: Path | None = None
    dir_filter: Path | None = None

    if arg and not force_reindex:
        if arg.lower().lstrip(".") in {"pdf", "docx", "pptx", "xlsx", "txt", "md", "html", "csv"}:
            ext_filter = arg.lower().lstrip(".")
        else:
            maybe_path = INPUT_DIR / arg
            if maybe_path.exists():
                single_target = maybe_path if maybe_path.is_file() else None
                dir_filter = maybe_path if maybe_path.is_dir() else None

    paths: List[Path] = (
        [single_target]
        if single_target
        else [p for p in (dir_filter or INPUT_DIR).rglob("*") if p.is_file()]
    )
    if ext_filter:
        paths = [p for p in paths if p.suffix.lower().lstrip(".") == ext_filter]
    if not paths:
        return "Nenalezeny Å¾Ã¡dnÃ© soubory odpovÃ­dajÃ­cÃ­ zadÃ¡nÃ­."

    # -- naÄÃ­tÃ¡nÃ­ & chunkovÃ¡nÃ­ ------------------------------------------------
    from langchain_unstructured import UnstructuredLoader  # lazy import

    new_docs: List[Document] = []
    reports: List[str] = []
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

    for path in paths:
        fid = _file_id(path)
        if not force_reindex and _already_indexed(fid):
            continue

        # 1ï¸âƒ£ naÄti dokument(y)
        try:
            raw_docs = UnstructuredLoader(str(path)).load()
        except Exception as exc:  # pragma: no cover
            reports.append(f"- {path.name}: âŒ NepodaÅ™ilo se naÄÃ­st ({exc})")
            continue

        # 2ï¸âƒ£ normalizuj na Document
        docs: List[Document] = []
        for item in raw_docs:
            if isinstance(item, Document):
                docs.append(item)
            elif isinstance(item, tuple) and len(item) == 2:
                content, meta = item
                docs.append(Document(page_content=str(content), metadata=dict(meta or {})))
            else:
                docs.append(Document(page_content=str(item), metadata={}))

        # 3ï¸âƒ£ metadata & sanitizace
        base_meta = {"source": "input_folder", "file_path": str(path), "file_id": fid}
        for d in docs:
            d.metadata.update(base_meta)
            _sanitize_metadata(d)

        # 4ï¸âƒ£ split + pÅ™idej do sbÃ­rky
        new_docs.extend(splitter.split_documents(docs))

        # 5ï¸âƒ£ shrnutÃ­ pro report
        summary = _summarise(" ".join(d.page_content for d in docs)[:8000], llm)
        reports.append(f"- {path.name}: {summary}")

    if new_docs:
        _vectorstore.add_documents(new_docs)  # Chroma se uloÅ¾Ã­ automaticky

    if not reports:
        return "Å½Ã¡dnÃ© novÃ© soubory nebyly naimportovÃ¡ny."
    return f"âœ… NaimportovÃ¡no {len(reports)} souborÅ¯:\n" + "\n".join(reports)

```


---

### `services\jira_content_service.py`

```python
# services/jira_content_service.py
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""OpenAI-powered helpers for drafting high-quality Jira issue content.

Public API
----------
enhance_idea(...)           -> Markdown for an *Idea* ticket
epic_from_idea(...)         -> Markdown for an *Epic* ticket
user_stories_for_epic(...)  -> Markdown list of INVEST-ready User Stories

"""

from __future__ import annotations

import os
import openai
from pathlib import Path
from typing import List
from jinja2 import Environment, FileSystemLoader, select_autoescape

from langchain_openai import ChatOpenAI

# â”€â”€ Disable OpenAI telemetry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ.setdefault("OPENAI_TELEMETRY", "0")
if hasattr(openai, "telemetry") and hasattr(openai.telemetry, "TelemetryClient"):
    openai.telemetry.TelemetryClient.capture = lambda *_, **__: None

# â”€â”€ LLM configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_MODEL = os.getenv("JIRA_CONTENT_MODEL", "gpt-4o")
_TEMPERATURE = float(os.getenv("JIRA_CONTENT_TEMPERATURE", "0.2"))

_llm = ChatOpenAI(model=_MODEL, temperature=_TEMPERATURE)

# â”€â”€ Jinja2 prostÅ™edÃ­ pro Å¡ablony â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_PROMPT_ENV = Environment(
    loader=FileSystemLoader(Path(__file__).resolve().parent.parent / "prompts"),
    autoescape=select_autoescape(disabled_extensions=("jinja2",)),
    trim_blocks=True,
    lstrip_blocks=True,
)

def _render(name: str, **kwargs) -> str:
    """Vyrenderuj zadanou .jinja2 Å¡ablonu s parametry."""
    return _PROMPT_ENV.get_template(name).render(**kwargs)

# â”€â”€ Internal helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _run(prompt: str) -> str:
    """Call the LLM synchronously and return trimmed text."""
    return _llm.invoke(prompt).content.strip()


# â”€â”€ High-level generators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def enhance_idea(
    summary: str,
    description: str | None = None,
    audience: str = "mixed",
    max_words: int = 360,
    ) -> str:

    prompt = _render(
        "idea.jinja2",
        summary=summary,
        description=description,
        audience=audience,
        max_words=max_words,
    )
    return _run(prompt)


def epic_from_idea(
        summary: str,
        description: str | None = None
        ) -> str:
    """
    Scale an Idea into a **Jira Epic**.  Outputs Markdown containing:
      â€¢ Epic Goal
      â€¢ Context
      â€¢ Definition of Done
      â€¢ Acceptance criteria
      â€¢ Out of scope
    """
    prompt = _render(
        "epic.jinja2",
        summary=summary,
        description=description,
    )
    return _run(prompt)


def user_stories_for_epic(
    epic_name: str,
    epic_description: str,
    count: int = 5,
) -> str:
    """
    Generate *count* INVEST-compliant User Stories for the given Epic.
    Each story includes title, user-story sentence, acceptance criteria
    and a T-shirt-size estimate.
    """
    prompt = _render(
        "user_stories.jinja2",
        epic_name=epic_name,
        epic_description=epic_description,
        count=count,
    )
    return _run(prompt)


__all__: List[str] = [
    "enhance_idea",
    "epic_from_idea",
    "user_stories_for_epic",
]




```


---

### `services\jira_service.py`

```python
"""Jira related service layer extracted from legacy jira_client module."""
from __future__ import annotations

from typing import Any, Dict, List, Sequence, Tuple
from datetime import datetime
from pathlib import Path
import json
import os
import time
import math
import inspect

from atlassian import Jira
from langchain_openai import OpenAIEmbeddings

__all__ = [
    "JiraClient",
    "find_duplicate_ideas",
    "JiraClientError",
    "_extract_text_from_adf",
]

_CFG_PATH = Path("config.json")
_RETRY_SLEEP = (0.5, 1.5, 3.0)  # exponential back-off in seconds


class JiraClientError(RuntimeError):
    """Raised when JiraClient cannot complete a request."""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utility helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _load_cfg() -> Dict[str, Any]:
    if _CFG_PATH.exists():
        with _CFG_PATH.open(encoding="utf-8") as fh:
            return json.load(fh).get("jira", {})
    return {}


def _fields_to_string(fields: Sequence[str] | str | None) -> str:
    if fields is None:
        return "*all"
    if isinstance(fields, str):
        return fields
    return ",".join(fields)


def _extract_text_from_adf(adf: Any) -> str:
    """Very small ADFâ†’plain-text converter (sufficient for descriptions)."""
    if adf is None:
        return ""
    stack: List[Any] = [adf]
    parts: List[str] = []
    while stack:
        n = stack.pop()
        if isinstance(n, dict):
            if n.get("type") == "text" and "text" in n:
                parts.append(n["text"])
            for child_key in ("content", "items"):
                if isinstance(n.get(child_key), list):
                    stack.extend(n[child_key])
        elif isinstance(n, list):
            stack.extend(n)
    return " ".join(parts).strip()


def _merge(issue: Dict[str, Any]) -> Dict[str, Any]:
    merged = {**issue.get("fields", {})}
    for k, v in issue.items():
        if k not in {"fields", "expand", "id", "key", "self"}:
            merged.setdefault(k, v)
    return merged


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main client â€“ public API only search_issues & get_issue
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class JiraClient:  # pylint: disable=too-few-public-methods
    """Typed, minimal wrapper around *atlassian-python-api*'s ``Jira``."""

    def __init__(
        self,
        url: str | None = None,
        email: str | None = None,
        token: str | None = None,
        *,
        max_retries: int = 3,
        timeout: int = 10,
    ) -> None:
        cfg = _load_cfg()
        url = url or cfg.get("url")
        email = email or cfg.get("user") or cfg.get("email")
        token = token or os.getenv("JIRA_AUTH_TOKEN")
        if not (url and email and token):
            raise ValueError(
                "Missing Jira credentials â€“ provide via arguments, config.json or env vars."
            )
        self._jira = Jira(url=url, username=email, password=token, cloud=True, timeout=timeout)
        self._retries = max_retries

    # ------------------------------------------------------------------ helpers
    def _call(self, func, *args, **kwargs):  # noqa: ANN001 â€“ 3rd-party sig
        for delay in (*_RETRY_SLEEP, None):
            try:
                return func(*args, **kwargs)
            except Exception as exc:  # noqa: BLE001 â€“ surface last
                if delay is None:
                    raise JiraClientError(str(exc)) from exc
                time.sleep(delay)

    # ------------------------------------------------------------------ public
    def search_issues(
        self,
        jql: str,
        *,
        max_results: int = 50,
        fields: Sequence[str] | str | None = None,
    ) -> List[Dict[str, Any]]:
        wanted = _fields_to_string(
            fields or ["summary", "status", "issuetype", "labels", "description"]
        )
        payload: Dict[str, Any] = self._call(
            self._jira.jql, jql, limit=max_results, fields=wanted
        )
        issues: List[Dict[str, Any]] = payload.get("issues", [])
        for iss in issues:
            fld = _merge(iss)
            if isinstance(fld.get("description"), (dict, list)):
                plain = _extract_text_from_adf(fld["description"])
                fld["description_plain"] = plain
                fld["description"] = plain
            iss["fields"] = fld
        return issues

    def get_issue(
        self,
        key: str,
        *,
        fields: Sequence[str] | str | None = None,
    ) -> Dict[str, Any]:
        wanted = _fields_to_string(fields or "*all")
        raw: Dict[str, Any] = self._call(self._jira.issue, key, fields=wanted)
        fld = _merge(raw)
        if isinstance(fld.get("description"), (dict, list)):
            plain = _extract_text_from_adf(fld["description"])
            fld["description_plain"] = plain
            fld["description"] = plain
        return {"key": key, "fields": fld}
    
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # JiraClient.update_issue â€“ jedinÃ½ veÅ™ejnÃ½ mutaÄnÃ­ entry-point
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def update_issue(
        self,
        key: str,
        data: Dict[str, Any] | None = None,
    ) -> None:
        """
        Update arbitrary fields of a Jira issue (napÅ™. description, labelsâ€¦).

        data = { "fields": {...}, "update": {...} }  # podle REST API
        """
        import inspect   # <- uÅ¾ je naimportovÃ¡n nahoÅ™e, pÅ™idejte jen pokud chybÃ­

        data = data or {}
        fields_param = data.get("fields")
        update_param = data.get("update")

        # 1ï¸âƒ£ Vybereme metodu (update_issue â€º edit_issue â€º Issue.update)
        if hasattr(self._jira, "update_issue"):
            func = self._jira.update_issue
        elif hasattr(self._jira, "edit_issue"):
            func = self._jira.edit_issue
        else:
            issue = self._call(self._jira.issue, key)
            func = issue.update

        # 2ï¸âƒ£ PÅ™ipravÃ­me payload pro â€starÅ¡Ã­â€œ signaturu (jedinÃ½ dict)
        payload: Dict[str, Any] = {}
        if fields_param is not None:
            payload["fields"] = fields_param
        if update_param is not None:
            payload["update"] = update_param

        # Pokud posÃ­lÃ¡me ADF (tj. description=dict), obejdeme SDK
        if (
            fields_param
            and isinstance(fields_param.get("description"), dict)  # ADF poznÃ¡me podle dict
        ):
            url = f"{self._jira.url}/rest/api/3/issue/{key}"
            #   self._jira.session je requests.Session() uÅ¾ pÅ™ihlÃ¡Å¡enÃ½ BasicAuth-Tokenem
            resp = self._jira.session.put(url, json={"fields": fields_param})
            if not resp.ok:
                raise JiraClientError(
                    f"Jira update failed: {resp.status_code} {resp.text}"
                )
            return

        # 3ï¸âƒ£ ZavolÃ¡me sprÃ¡vnou signaturu podle toho, co metoda opravdu umÃ­
        sig = inspect.signature(func)
        try:
            if "fields" in sig.parameters:
                # NovÄ›jÅ¡Ã­ atlassian-python-api (>=3.41) â€“ podporuje pojmenovanÃ© parametry
                self._call(func, key, fields=fields_param, update=update_param)
            else:
                # StarÅ¡Ã­ verze â€“ oÄekÃ¡vÃ¡ jedinÃ½ slovnÃ­k payload
                self._call(func, key, payload)
        except Exception as exc:                 # noqa: BLE001
            raise JiraClientError(f"Jira update failed: {exc}") from exc





# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Embed-cache + normalizace â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_ABBREV = {
    "2fa": "two factor authentication",
    "mfa": "multi factor authentication",
    "sso": "single sign on",
    # pÅ™idej dalÅ¡Ã­ zkratky podle potÅ™ebyâ€¦
}


def _normalize(text: str) -> str:
    """Lower-case + rozbalÃ­ bÄ›Å¾nÃ© zkratky pÅ™ed embedovÃ¡nÃ­m."""
    t = text.lower()
    for short, full in _ABBREV.items():
        t = t.replace(short, full)
    return t


_CACHE_TTL = 300  # sekund
_EMBED_CACHE: dict[str, tuple[list[float], float]] = {}
_MODEL: OpenAIEmbeddings | None = None


def _get_model() -> OpenAIEmbeddings:
    global _MODEL
    if _MODEL is None:
        _MODEL = OpenAIEmbeddings(model="text-embedding-3-small")
    return _MODEL


def _cached_embedding(text: str) -> list[float]:
    """Embed normalizovanÃ½ text a vÃ½sledek podrÅ¾ v pamÄ›ti max. 5 minut."""
    text = _normalize(text)
    now = time.time()
    if (cache := _EMBED_CACHE.get(text)) and now - cache[1] < _CACHE_TTL:
        return cache[0]

    emb = _get_model().embed_query(text)
    _EMBED_CACHE[text] = (emb, now)
    return emb


def _cosine(u: Sequence[float], v: Sequence[float]) -> float:
    dot = sum(a * b for a, b in zip(u, v))
    nu = math.sqrt(sum(a * a for a in u))
    nv = math.sqrt(sum(b * b for b in v))
    return 0.0 if not nu or not nv else dot / (nu * nv)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Public helper â€“ DUPLICATE CHECK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-

def find_duplicate_ideas(
    summary: str,
    description: str | None = None,
    threshold: float = 0.75,
) -> List[str]:
    """Return keys of Jira Ideas similar to the provided summary."""
    if not 0.0 <= threshold <= 1.0:
        raise ValueError("threshold must be between 0 and 1")

    query_text = summary if description is None else f"{summary} {description}"
    query_vec = _cached_embedding(query_text)

    client = JiraClient()
    jql = "issuetype = Idea AND resolution = Unresolved"
    issues = client.search_issues(
        jql, max_results=200, fields=["summary", "description"]
    )

    scored: List[Tuple[str, float]] = []
    for issue in issues:
        key = issue["key"]
        fld = issue.get("fields", {})
        idea_text = f"{fld.get('summary', '')} {fld.get('description', '')}".strip()
        if not idea_text:
            continue
        sim = _cosine(query_vec, _cached_embedding(idea_text))
        if sim >= threshold:
            scored.append((key, sim))

    scored.sort(key=lambda x: x[1], reverse=True)
    return [k for k, _ in scored]

```


---

### `services\rag_service.py`

```python
"""Utilities for working with the RAG vector store."""
from __future__ import annotations

import os
import json
from pathlib import Path
from datetime import datetime
from typing import List

from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import DirectoryLoader, ConfluenceLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Helper: Robust TXT Saver â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUTPUT_DIR = Path("output")
OUTPUT_DIR.mkdir(exist_ok=True)


def save_text_to_file(data: str, prefix: str = "research") -> str:
    """Save *data* into a new TXT file and report the path."""
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"{prefix}_{timestamp}.txt"
    path = OUTPUT_DIR / filename
    path.write_text(data, encoding="utf-8")
    return f"Data saved to {path.as_posix()}"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Vector store builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€--

def build_vectorstore(docs_path: str = "./data", persist_directory: str = "rag_chroma_db") -> None:
    """Create a Chroma vector store from text documents."""
    load_dotenv()
    loader = DirectoryLoader(docs_path, glob="**/*.txt")
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(documents)
    embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    Chroma.from_documents(documents=chunks, embedding=embedding_model, persist_directory=persist_directory)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RAG Retriever over Chroma â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-
CHROMA_DIR = "rag_chroma_db"
_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
_retriever = Chroma(persist_directory=CHROMA_DIR, embedding_function=_embeddings).as_retriever(search_kwargs={"k": 4})


def rag_lookup(query: str) -> str:
    """Return up to 4 relevant documents from the vector store."""
    docs: List[Document] = _retriever.invoke(query)
    if not docs:
        return "Å½Ã¡dnÃ© internÃ­ dokumenty se k dotazu nenaÅ¡ly."
    return "\n\n".join(d.page_content for d in docs)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Confluence loader ----------------------------------

def load_confluence_pages(config_path: Path | str = "config.json") -> None:
    """Load Confluence pages referenced in *config_path* into the vector store."""
    load_dotenv()
    token = os.getenv("JIRA_AUTH_TOKEN")
    if not token:
        raise RuntimeError("Missing JIRA_AUTH_TOKEN in environment")

    cfg = json.loads(Path(config_path).read_text(encoding="utf-8"))
    conf_cfg = cfg.get("confluence", {})
    base_url = conf_cfg.get("base_url")
    user = conf_cfg.get("user")
    ancestor_ids = conf_cfg.get("ancestor_ids", [])
    if not ancestor_ids:
        return

    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma(persist_directory=CHROMA_DIR, embedding_function=embeddings)
    existing_ids = {
        str(m.get("page_id") or m.get("id"))
        for m in vectorstore._collection.get(include=["metadatas"])["metadatas"]
        if m.get("page_id") or m.get("id")
    }

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    new_chunks: List[Document] = []

    for anc in ancestor_ids:
        loader = ConfluenceLoader(
            url=base_url,
            username=user,
            api_key=token,
            cql=f"ancestor={anc}",
            include_archived_content=False,
            include_restricted_content=False,
        )
        docs = loader.load()
        fresh = []
        for doc in docs:
            pid = str(doc.metadata.get("id") or doc.metadata.get("page_id"))
            if pid in existing_ids:
                continue
            doc.metadata["page_id"] = pid
            fresh.append(doc)
        if not fresh:
            continue
        new_chunks.extend(splitter.split_documents(fresh))

    if new_chunks:
        vectorstore.add_documents(new_chunks)


```


---

### `services\web_service.py`

```python
"""Web search helpers split from the legacy tools module."""
from __future__ import annotations

import os
import openai
from tavily import TavilyClient
from langchain_community.tools import DuckDuckGoSearchRun, WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

# Optâ€‘out from OpenAI telemetry
os.environ.setdefault("OPENAI_TELEMETRY", "0")
if hasattr(openai, "telemetry") and hasattr(openai.telemetry, "TelemetryClient"):
    openai.telemetry.TelemetryClient.capture = lambda *_, **__: None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ General Web Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_duck = DuckDuckGoSearchRun()


def search_web(query: str) -> str:
    """Return DuckDuckGo search snippets for *query*."""
    return _duck.run(query)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Wikipedia Snippet ----------------------------------
_api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=400)
_wiki_runner = WikipediaQueryRun(api_wrapper=_api_wrapper)


def wiki_snippet(query: str) -> str:
    """Return a short Wikipedia summary for *query*."""
    return _wiki_runner.run(query)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tavily Semantic Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_TAVILY_KEY = os.getenv("TAVILY_API_KEY")
_client: TavilyClient | None = TavilyClient(api_key=_TAVILY_KEY) if _TAVILY_KEY else None


def tavily_search(query: str) -> str:
    """Perform semantic search via Tavily if configured."""
    if _client is None:
        return "Tavily nenÃ­ nakonfigurovÃ¡no (chybÃ­ TAVILY_API_KEY)."
    try:
        raw = _client.search(query=query, max_results=6)
    except Exception as exc:  # pragma: no cover - external call
        return f"Tavily search selhalo: {exc}"

    results = raw.get("results", [])
    if not results:
        return "Tavily nic nenaÅ¡lo."
    snippets = [f"- {r['url']}\n  {r['content'][:400].strip()}â€¦" for r in results]
    return "\n\n".join(snippets)

```


---

### `services\__init__.py`

```python
# services/__init__.py
"""Service layer consolidating Jira, RAG and web helpers."""
from .jira_service import JiraClient, JiraClientError, find_duplicate_ideas, _extract_text_from_adf
from .rag_service import (
    save_text_to_file,
    build_vectorstore,
    rag_lookup,
    load_confluence_pages,
)
from .web_service import search_web, wiki_snippet, tavily_search
from .jira_content_service import (
    enhance_idea,
    epic_from_idea,
    user_stories_for_epic,
)
from .input_loader import process_input_files

__all__ = [
    "JiraClient",
    "JiraClientError",
    "find_duplicate_ideas",
    "save_text_to_file",
    "build_vectorstore",
    "rag_lookup",
    "load_confluence_pages",
    "search_web",
    "wiki_snippet",
    "tavily_search",
    "_extract_text_from_adf",
    "jira_content_tools",
    "process_input_files",
]

```


---

### `tools\input_loader.py`

```python
# tools/input_loader.py
"""
LangChain Tool: process_input_files
-----------------------------------
Wrapper, kterÃ½ zpracuje volitelnÃ½ `arg` od agenta a pÅ™epoÅ¡le ho do sluÅ¾ebnÃ­
funkce.  DÃ­ky tomu mÅ¯Å¾e agent jemnÄ› Å™Ã­dit import (viz docstring v services).
"""
from langchain.tools import Tool
from services.input_loader import process_input_files


def _process_input_files(arg: str = "") -> str:
    """Proxy, kterÃ¡ pÅ™edÃ¡ argument dÃ¡l do business-logiky."""
    return process_input_files(arg)


process_input_tool = Tool(
    name="process_input_files",
    func=_process_input_files,
    description=(
        """
        Purpose
        -------
        Scan the local **`./input`** drop-folder, import every file that matches the
        given filter into the Chroma vector database so RAG queries can reference
        them, and return a one-sentence Markdown summary per file.

        Parameters
        ----------
        arg : str, optional â€” controls behaviour

            â€¢ ""            â†’ default full scan (skip already-indexed files)  
            â€¢ "force"       â†’ re-index **everything** (ignore fingerprints)  
            â€¢ "pdf" / ".pdf"â†’ import *only* files with this extension  
            â€¢ "subdir/foo"  â†’ limit scan to the given (sub)directory  
            â€¢ "file.ext"    â†’ import exactly the specified file  

        Returns
        -------
        Markdown report, e.g.:

            âœ… Imported 2 files:
            - report_Q3.pdf: Overview of KPI results for Q3 2024 â€¦
            - shifts.xlsx  : Daily OEE and breakdown statistics â€¦

        Use this tool whenever the user wants to make newly dropped local files
        available to the retrieval pipeline or asks â€œwhat did I just upload?â€.
        """
    ),
)

__all__ = ["process_input_tool"]

```


---

### `tools\jira_content_tools.py`

```python
# tools/jira_content_tools.py
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
Tool-layer wrapper for the OpenAI-powered Jira content helpers.

These tools turn fuzzy inputs into production-ready Jira tickets in
professional English, following agile / software-engineering best-practices.

Public attribute
----------------
ALL_TOOLS : list[StructuredTool]
    Import and add to your agent's tool list, e.g.:

    from tools.jira_content_tools import ALL_TOOLS
    agent = initialize_agent(
        tools=ALL_TOOLS + other_tools,
        ...
    )
"""
from __future__ import annotations

from typing import List, Literal

from langchain.tools import StructuredTool
from pydantic import BaseModel, Field

from services.jira_content_service import (
    enhance_idea as _enhance_idea,
    epic_from_idea as _epic_from_idea,
    user_stories_for_epic as _user_stories_for_epic,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Enhance Idea â†’ Jira Idea
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class EnhanceIdeaInput(BaseModel):
    """Arguments for the *enhance_idea* tool."""
    summary: str = Field(
        ...,
        description="One-line product idea summary. Keep it short and action-oriented.",
        examples=["Predictive maintenance for packaging line"],
    )
    description: str | None = Field(
        default=None,
        description="Optional free-form description copied from stakeholder notes.",
        examples=[
            "Operators complain about frequent unplanned stops. "
            "We should investigate sensor trends, forecast failures and schedule maintenance."
        ],
    )
    audience: Literal["business", "technical", "mixed"] = Field(
        default="mixed",
        description="Target audience that will read the Idea (affects tone).",
        examples=["business"],
    )
    max_words: int | None = Field(
        default=360,
        description="Hard limit for total word count of the generated description.",
        examples=[100],
    )


def _enhance_idea_tool(
    *,
    summary: str,
    description: str | None = None,
    audience: str = "mixed",
    max_words: int | None = 120,
    ) -> str:
    """Convert a rough product idea into a polished **Jira Idea** ticket body.

    Output format (Markdown):
    - ## Problem
    - ## Proposed solution
    - ## Business value (bullets)
    - ## Acceptance criteria (G/W/T bullets)

    Always writes in professional British English, avoids fluff, and
    adheres to company style (second-level headings, max 5 ACs)."""

    return _enhance_idea(
        summary=summary,
        description=description,
        audience=audience,
        max_words=max_words or 120,
    )


enhance_idea_tool = StructuredTool.from_function(
    name="enhance_idea",
    description=(
        "Transform a **raw product idea** (summary + optional notes) into a concise, "
        "board-ready *Jira Idea* body in Markdown.\n\n"
        "**When to call**  â€¢ Any time stakeholder wording is informal, incomplete, "
        "in Czech, or otherwise unfit for an executive audience.\n\n"
        "**Output**  â€¢ Four second-level headings â€” *Problem*, *Proposed solution*, "
        "*Business value*, *Acceptance criteria* â€” in professional British English, "
        "capped at `max_words` (default 360).\n\n"
        "**Args**\n"
        "â€‚â€¢ `summary`â€‚(STR, required) â€“ one-line tagline.\n"
        "â€‚â€¢ `description`â€‚(STR, optional) â€“ stakeholder context.\n"
        "â€‚â€¢ `audience`â€‚(ENUM) â€“ \"business\", \"technical\", or \"mixed\"; "
        "controls tone and terminology.\n"
        "â€‚â€¢ `max_words`â€‚(INT) â€“ hard length limit.\n\n"
        "Avoid jargon, keep headings exactly as above, respect the word limit."
    ),
    func=_enhance_idea_tool,
    args_schema=EnhanceIdeaInput,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Idea â†’ Epic
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class EpicFromIdeaInput(BaseModel):
    """Arguments for the *epic_from_idea* tool."""
    summary: str = Field(
        ...,
        description="Idea summary that will become the epic goal.",
        examples=["Predictive maintenance for packaging line"],
    )
    description: str | None = Field(
        default=None,
        description="Optional idea description giving context/problem statement.",
    )


def _epic_from_idea_tool(*, summary: str, description: str | None = None) -> str:
    """Produce a complete **Jira Epic** template in Markdown.

    Sections:
    - Epic Goal
    - Context
    - Definition of Done (bullets)
    - Acceptance criteria (G/W/T, â‰¤7)
    - Out of scope

    The text is direct, neutral, no marketing adjectives."""
    return _epic_from_idea(summary, description)


epic_from_idea_tool = StructuredTool.from_function(
    name="epic_from_idea",
    description=(
        "Expand a validated Idea into a comprehensive *Epic* draft "
        "ready for backlog refinement, with DoD & ACs."
    ),
    func=_epic_from_idea_tool,
    args_schema=EpicFromIdeaInput,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Epic â†’ User Stories
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class UserStoriesForEpicInput(BaseModel):
    """Arguments for the *user_stories_for_epic* tool."""
    epic_name: str = Field(
        ...,
        description="Exact Jira Epic name / goal.",
        examples=["Predictive maintenance platform (Phase I)"],
    )
    epic_description: str = Field(
        ...,
        description="Full epic description (context, DoD, ACs).",
    )
    count: int = Field(
        default=5,
        ge=1,
        le=15,
        description="How many user stories to generate. Default 5.",
    )


def _user_stories_for_epic_tool(
    *, epic_name: str, epic_description: str, count: int = 5
) -> str:
    """Generate INVEST-compliant **User Stories** for the given epic in czech language.

    For each story:
    - Title
    - One-paragraf abstract
    - â€˜As a â€¦ I want â€¦ so that â€¦â€™
    - Acceptance criteria (G/W/T)
    - T-shirt estimate"""
    return _user_stories_for_epic(epic_name, epic_description, count)


user_stories_for_epic_tool = StructuredTool.from_function(
    name="user_stories_for_epic",
    description=(
        """Generate INVEST-compliant **User Stories** for the given epic in czech language.

        For each story:
        - Title
        - One-paragraf abstract
        - â€˜As a â€¦ I want â€¦ so that â€¦â€™
        - Acceptance criteria (G/W/T)
        - T-shirt estimate"""
    ),
    func=_user_stories_for_epic_tool,
    args_schema=UserStoriesForEpicInput,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Convenience export
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ALL_TOOLS: List[StructuredTool] = [
    enhance_idea_tool,
    epic_from_idea_tool,
    user_stories_for_epic_tool,
]

__all__ = [
    "enhance_idea_tool",
    "epic_from_idea_tool",
    "user_stories_for_epic_tool",
    "ALL_TOOLS",
]

```


---

### `tools\jira_tools.py`

```python
from __future__ import annotations

import os
import re
import openai
from typing import List, Optional

from langchain.tools import StructuredTool
from pydantic import BaseModel, Field
from requests.exceptions import HTTPError
import difflib

from services import JiraClient, find_duplicate_ideas, _extract_text_from_adf

# Optâ€‘out from OpenAI telemetry
os.environ.setdefault("OPENAI_TELEMETRY", "0")
if hasattr(openai, "telemetry") and hasattr(openai.telemetry, "TelemetryClient"):
    openai.telemetry.TelemetryClient.capture = lambda *_, **__: None

# Single Jira client instance ---------------------------------------------------
_JIRA = JiraClient()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ JIRA Ideas Retriever â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€--
class JiraIdeasInput(BaseModel):
    """Optional keyword filter for JIRA Ideas."""

    keyword: Optional[str] = Field(
        default=None,
        description=(
            "KlÃ­ÄovÃ© slovo pro filtrovÃ¡nÃ­ Ideas podle summary/description. Pokud None, vrÃ¡tÃ­ vÅ¡e."
        ),
    )


def _jira_ideas_struct(keyword: Optional[str] = None) -> str:
    try:
        issues = _JIRA.search_issues("project = P4 ORDER BY created DESC", max_results=100)
    except Exception as exc:  # pragma: no cover â€“ user-facing path only
        return f"Chyba pÅ™i naÄÃ­tÃ¡nÃ­ JIRA: {exc}"

    if not issues:
        return "Nenalezeny Å¾Ã¡dnÃ© JIRA Ideas."

    def _plain(issue):
        f = {**issue.get("fields", {}), **{k: v for k, v in issue.items() if k != "fields"}}
        return {
            "key": issue["key"],
            "summary": f.get("summary", ""),
            "description": f.get("description_plain") or f.get("description", ""),
            "status": f.get("status", {}).get("name", ""),
        }

    ideas = [_plain(i) for i in issues]

    if keyword:
        kw = keyword.lower()
        ideas = [i for i in ideas if kw in (i["summary"] + i["description"]).lower()]
        if not ideas:
            return f"Å½Ã¡dnÃ© Ideas neobsahujÃ­ klÃ­ÄovÃ© slovo '{keyword}'."

    lines = [
        f"{i['key']} | {i['status']} | {i['summary']}\n" f"{i['description'] or '- Å¾Ã¡dnÃ½ popis -'}"
        for i in ideas
    ]
    return "\n\n".join(lines)


jira_ideas = StructuredTool.from_function(
    func=_jira_ideas_struct,
    name="jira_ideas_retriever",
    description=(
        """
        Purpose
        -------
        List items from the P4 Jira â€œIdeasâ€ backlog in a readable table-like text form.
        Useful for spotting duplicates manually, expanding an idea into deeper detail,
        or extracting candidate acceptance criteria.

        Parameters
        ----------
        keyword : str | None â€“ optional free-text filter applied to summary + description
                (case-insensitive).  If None, returns the entire backlog (max 100).

        Returns
        -------
        For each match: "KEY | Status | Summary" on one line, followed by the description
        on the next line.
        """
    ),
    args_schema=JiraIdeasInput,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Jira Issue Detail â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€--
class JiraIssueDetailInput(BaseModel):
    """Schema for ``jira_issue_detail`` tool."""

    key: str = Field(..., description="Jira key, e.g. P4-123")


def _format_acceptance_criteria(text: str) -> str:
    pattern = re.compile(r"^\s*(?:\*|-)?\s*(Given|When|Then)\b.*", re.IGNORECASE)
    items = [ln.strip() for ln in text.splitlines() if pattern.match(ln)]
    return "\n".join(f"- {ln.lstrip('*- ').strip()}" for ln in items)


def _jira_issue_detail(key: str) -> str:
    try:
        issue = _JIRA.get_issue(
            key,
            fields=[
                "summary",
                "status",
                "issuetype",
                "labels",
                "description",
                "subtasks",
                "comment",
            ],
        )
    except HTTPError as http_exc:
        code = getattr(http_exc.response, "status_code", None)
        if code in (403, 404) or "does not exist" in str(http_exc).lower():
            return f"Issue {key} neexistuje nebo k nÄ›mu nemÃ¡te pÅ™Ã­stup."
        return f"HTTP chyba pÅ™i naÄÃ­tÃ¡nÃ­ {key}: {http_exc}"
    except Exception as exc:  # pragma: no cover â€“ other unexpected error
        return f"Chyba pÅ™i naÄÃ­tÃ¡nÃ­ issue {key}: {exc}"

    f = {**issue.get("fields", {}), **{k: v for k, v in issue.items() if k != "fields"}}

    title = f.get("summary", "â€”")
    status = f.get("status", {}).get("name", "â€”")
    itype = f.get("issuetype", {}).get("name", "â€”")
    labels = ", ".join(f.get("labels") or []) or "â€”"

    from services import _extract_text_from_adf

    raw_desc = f.get("description")
    description_src = f.get("description_plain") or f.get("description")
    description = (
        _extract_text_from_adf(raw_desc)
        if isinstance(description_src, (dict, list))
        else (raw_desc or "â€”")
    ).strip()

    ac_block = _format_acceptance_criteria(description)

    subtasks = f.get("subtasks") or []
    sub_lines = [
        f"- {st['key']} â€“ {st.get('fields', {}).get('summary', '')}".rstrip()
        for st in subtasks
    ]

    comments = sorted(
        (f.get("comment", {}).get("comments") or []),
        key=lambda c: c.get("created", ""),
        reverse=True,
    )[:3]

    def _fmt_date(ts: str) -> str:
        return ts.split("T")[0] if ts else "â€”"

    com_lines = [
        f"- **{c.get('author', {}).get('displayName', 'Unknown')}** "
        f"({_fmt_date(c.get('created'))}): {c.get('body', '').strip()}"
        for c in comments
    ]

    parts: list[str] = [
        f"**{key} â€“ {title}**",
        f"Status: {status} | Type: {itype} | Labels: {labels}",
        "",
        "### Description",
        description or "â€”",
    ]
    if ac_block:
        parts += ["", "### Acceptance Criteria", ac_block]
    if sub_lines:
        parts += ["", "### Sub-tasks", *sub_lines]
    if com_lines:
        parts += ["", "### Latest Comments", *com_lines]

    return "\n".join(parts).strip()


jira_issue_detail = StructuredTool.from_function(
    func=_jira_issue_detail,
    name="jira_issue_detail",
    description=(
        """
        Purpose
        -------
        Fetch a single Jira issue and format it as rich Markdown, giving a 360Â° snapshot
        for rapid context-building.

        Parameters
        ----------
        key : str â€“ Jira key, e.g. "P4-24".

        Output sections
        ---------------
        â€¢ Summary line
        â€¢ Status | Type | Labels
        â€¢ Full Description (ADF converted to text)
        â€¢ Acceptance Criteria (auto-extracted Given/When/Then, if present)
        â€¢ Sub-tasks list
        â€¢ Up to three latest comments with authors and dates
        """
    ),
    args_schema=JiraIssueDetailInput,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Jira Duplicate-Idea Checker (Structured) --------------------
class DuplicateIdeasInput(BaseModel):
    summary: str = Field(..., description="KrÃ¡tkÃ½ popis/summary novÃ© Idea.")
    description: str | None = Field(
        default=None,
        description="(VolitelnÃ©) DelÅ¡Ã­ popis nÃ¡padu â€“ zahrne se do kontroly duplicity.",
    )
    threshold: float = Field(
        default=0.8,
        ge=0,
        le=1,
        description="PrÃ¡h kosinovÃ© podobnosti 0-1 (vÃ½chozÃ­ 0.8).",
    )


def _duplicate_ideas(
    summary: str, description: str | None = None, threshold: float = 0.8
) -> str:
    try:
        matches = find_duplicate_ideas(summary, description, threshold)
    except ValueError as exc:  # invalid threshold
        return f"NeplatnÃ½ parametr `threshold`: {exc}"

    if not matches:
        return "Å½Ã¡dnÃ© potenciÃ¡lnÃ­ duplicity nad danÃ½m prahem nenalezeny."
    return "MoÅ¾nÃ© duplicitnÃ­ nÃ¡pady: " + ", ".join(matches)


jira_duplicates = StructuredTool.from_function(
    func=_duplicate_ideas,
    name="jira_duplicate_idea_checker",
    description=(
        """
        Purpose
        -------
        Detect whether a *new* idea is likely a **duplicate** of an existing P4 Jira Idea
        using cosine similarity of OpenAI embeddings.

        Parameters
        ----------
        summary     : str   (required) â€“ one-line headline of the proposed idea.
        description : str | None (optional) â€“ longer text; concatenated with summary for
                    embedding.
        threshold   : float       (optional, 0-1, default 0.8) â€“ similarity cutoff;
                    lower for fuzzier matches.

        Returns
        -------
        Either "No potential duplicates above threshold"
        OR a comma-separated list of Jira keys ordered by similarity (highest first).

        Implementation notes
        --------------------
        â€¢ Uses text-embedding-3-small.
        â€¢ Performs acronym expansion (e.g. 2FA â†’ "two factor authentication").
        â€¢ Considers *summary + description* on both sides.
        """
    ),
    args_schema=DuplicateIdeasInput,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Jira Update-Description Tool  (human-in-the-loop commit)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class UpdateDescriptionInput(BaseModel):
    """Arguments for *jira_update_description* tool."""
    key: str = Field(..., description="Issue key, e.g. 'P4-123'.")
    new_description: str = Field(
        ...,
        description="Entire new description (plain text nebo Markdown â€“ "
                    "Jira ho automaticky pÅ™evede).",
    )
    confirm: bool = Field(
        default=False,
        description=(
            "âš ï¸  SECURITY SWITCH â€“ musÃ­ bÃ½t **True**, aby se zmÄ›na zapsala do Jira. "
            "Pokud False (default), nÃ¡stroj pouze zobrazÃ­ diff a poÅ¾Ã¡dÃ¡ o potvrzenÃ­."
        ),
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _jira_update_description(
    *, key: str, new_description: str, confirm: bool = False
) -> str:
    """
    Two-step safe update of *description* field:

    1. confirm=False â†’ diff preview
    2. confirm=True  â†’ actual write (now with correct ADF!)
    """
    try:
        old_issue = _JIRA.get_issue(key, fields=["description"])
    except Exception as exc:
        return f"âŒ Nelze naÄÃ­st issue {key}: {exc}"

    old_raw = old_issue.get("fields", {}).get("description") or ""
    old_desc = (
        _extract_text_from_adf(old_raw)
        if isinstance(old_raw, (dict, list))
        else str(old_raw)
    ).strip()

    new_desc = new_description.strip()

    # Step 1 â€“ nÃ¡hled diffu
    if not confirm:
        diff = "\n".join(
            difflib.unified_diff(
                old_desc.splitlines(),
                new_desc.splitlines(),
                fromfile="aktuÃ¡lnÃ­",
                tofile="navrhovanÃ©",
                lineterm="",
            )
        ) or "*Å½Ã¡dnÃ½ rozdÃ­l*"
        return (
            f"### NÃ¡hled zmÄ›ny popisu pro **{key}**\n"
            f"```diff\n{diff}\n```\n"
            "Toto je **pouze nÃ¡hled** â€“ nic nebylo uloÅ¾eno.\n"
            "Chcete-li zmÄ›nu potvrdit, znovu spusÅ¥te `jira_update_description` "
            "se stejnÃ½mi parametry a `confirm=True`."
        )

    # Step 2 â€“ skuteÄnÃ½ zÃ¡pis pÅ™es ADF
    if old_desc == new_desc:
        return "â„¹ï¸ NovÃ½ popis je identickÃ½ se stÃ¡vajÃ­cÃ­m â€“ nic se nezmÄ›nilo."

    # â†’ zde zabalÃ­me plain text do ADF
    adf_doc = {
        "version": 1,
        "type": "doc",
        "content": [
            {
                "type": "paragraph",
                "content": [
                    {"type": "text", "text": new_desc}
                ],
            }
        ],
    }

    try:
        _JIRA.update_issue(key, {"fields": {"description": adf_doc}})
    except Exception as exc:
        return f"âŒ Aktualizace selhala: {exc}"

    return f"âœ… Popis issue **{key}** byl ÃºspÄ›Å¡nÄ› aktualizovÃ¡n."
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

jira_update_description = StructuredTool.from_function(
    func=_jira_update_description,
    name="jira_update_description",
    description=(
        """
        Safe, human-confirmed update of a Jira issueâ€™s **Description** field.

        Typical workflow
        ----------------
        1. Call tool **without** `confirm` â†’ diff preview is returned.  
        2. If the preview is correct, call again with `confirm=True` to commit.

        Parameters
        ----------
        key            : str   â€“ issue key.  
        new_description: str   â€“ full replacement body (plain/Markdown).  
        confirm        : bool  â€“ must be True to actually save.

        Returns
        -------
        â€¢ Preview (`confirm=False`) â€“ unified diff in ```diff``` block.  
        â€¢ Commit   (`confirm=True`) â€“ success / error message.
        """
    ),
    args_schema=UpdateDescriptionInput,
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Jira Child-Issues Retriever â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ChildIssuesInput(BaseModel):
    """VrÃ¡tÃ­ pÅ™Ã­mÃ© child issues (Stories, Tasksâ€¦) pod zadanÃ½m issue/Epicem."""
    key: str = Field(..., description="Jira key nadÅ™azenÃ©ho issue, napÅ™. 'P4-123'.")

def _jira_child_issues(key: str) -> str:
    """
    Najde vÅ¡echny *pÅ™Ã­mÃ©* potomky (parent/â€Epic Linkâ€œ) zadanÃ©ho issue.

    â€¢ Pro Company-managed projekty platÃ­ JQL `parent = KEY`
    â€¢ Pro Classic projekty (starÅ¡Ã­ Epic Link) `\"Epic Link\" = KEY`
      â†’ kombinujeme obÄ› podmÃ­nky, abychom pokryli obÄ› varianty.
    """
    try:
        jql = f'parent = "{key}" OR "Epic Link" = "{key}"'
        issues = _JIRA.search_issues(
            jql,
            max_results=100,
            fields=["summary", "status", "issuetype"],
        )
    except Exception as exc:                        # pragma: no cover
        return f"Chyba pÅ™i naÄÃ­tÃ¡nÃ­ JIRA: {exc}"

    if not issues:
        return f"Issue {key} nemÃ¡ Å¾Ã¡dnÃ© pÅ™Ã­mÃ© child issues."

    def _row(i):
        f = i.get("fields", {})
        t = f.get("issuetype", {}).get("name", "â€”")
        s = f.get("status", {}).get("name", "â€”")
        return f"{i['key']} | {t} | {s} | {f.get('summary', '')}"

    return "\n".join(_row(i) for i in issues)

jira_child_issues = StructuredTool.from_function(
    func=_jira_child_issues,
    name="jira_child_issues",
    description=(
        """
        Purpose
        -------
        ZobrazÃ­ seznam vÅ¡ech pÅ™Ã­mÃ½ch child issues (Stories, Tasks, Bugsâ€¦)
        pod zadanÃ½m nadÅ™azenÃ½m issue (typicky Epic).

        Parameters
        ----------
        key : str â€“ Jira key, napÅ™. "P4-42".

        Returns
        -------
        KaÅ¾dÃ½ Å™Ã¡dek: "KEY | IssueType | Status | Summary".
        """
    ),
    args_schema=ChildIssuesInput,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ JIRA Issue-Links Explorer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from typing import List               # pokud uÅ¾ mÃ¡te, tento import mÅ¯Å¾ete smazat
from pydantic import BaseModel, Field
from langchain.tools import StructuredTool

class IssueLinksInput(BaseModel):
    """VypÃ­Å¡e vÅ¡echny vazby (issue links) k danÃ©mu Jira issue."""
    key: str = Field(..., description="Jira key, napÅ™. 'P4-42'.")

def _jira_issue_links(key: str) -> str:
    """
    VrÃ¡tÃ­ kompletnÃ­ seznam vÅ¡ech propojenÃ½ch issues a typ vazby.

    â€¢ Pro kaÅ¾dou linku rozliÅ¡Ã­ smÄ›r (inward/outward) podle Jiry.
    â€¢ FormÃ¡t: "KEY | Relation | Type | Status | Summary"
    """
    try:
        issue = _JIRA.get_issue(key, fields=["issuelinks"])
    except Exception as exc:                         # pragma: no cover
        return f"Chyba pÅ™i naÄÃ­tÃ¡nÃ­ JIRA: {exc}"

    links = issue.get("fields", {}).get("issuelinks", [])
    if not links:
        return f"Issue {key} nemÃ¡ Å¾Ã¡dnÃ© vazby."

    rows: List[str] = []
    for ln in links:
        ltype = ln.get("type", {})
        relation = ltype.get("name", "â€”")
        # rozliÅ¡Ã­me smÄ›r
        if "outwardIssue" in ln:
            other = ln["outwardIssue"]
            relation = ltype.get("outward") or relation
        elif "inwardIssue" in ln:
            other = ln["inwardIssue"]
            relation = ltype.get("inward") or relation
        else:
            continue

        okey = other.get("key", "???")
        f = other.get("fields", {})
        otype = f.get("issuetype", {}).get("name", "â€”")
        ostatus = f.get("status", {}).get("name", "â€”")
        osummary = f.get("summary", "")

        rows.append(f"{okey} | {relation} | {otype} | {ostatus} | {osummary}")

    return "\n".join(rows)


jira_issue_links = StructuredTool.from_function(
    func=_jira_issue_links,
    name="jira_issue_links",
    description=(
        """
        Purpose
        -------
        Returns all relations (links) of selected Jira issue â€“ duplicates, relates-to,
        blocks, etc. Every relation has information direction (inward/outward).

        Parameters
        ----------
        key : str â€“ Jira key (for example "P4-42").

        Returns
        -------
        One line per relation:
        "KEY | Relation | IssueType | Status | Summary"
        """
    ),
    args_schema=IssueLinksInput,
)


__all__ = [
    "jira_ideas",
    "jira_issue_detail",
    "jira_duplicates",
    "_JIRA",
    "_jira_issue_detail",
    "jira_update_description",
    "jira_child_issues",
    "jira_issue_links",
]

```


---

### `tools\rag_tools.py`

```python
from __future__ import annotations

import os
import openai
from datetime import datetime
from pathlib import Path
from typing import List

from langchain.tools import Tool
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document

# Optâ€‘out from OpenAI telemetry
os.environ.setdefault("OPENAI_TELEMETRY", "0")
if hasattr(openai, "telemetry") and hasattr(openai.telemetry, "TelemetryClient"):
    openai.telemetry.TelemetryClient.capture = lambda *_, **__: None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Helper: Robust TXT Saver â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUTPUT_DIR = Path("output")
OUTPUT_DIR.mkdir(exist_ok=True)


def _save_to_txt(data: str, prefix: str = "research") -> str:
    """Save *data* into a new TXT file and report the path."""
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"{prefix}_{timestamp}.txt"
    path = OUTPUT_DIR / filename
    path.write_text(data, encoding="utf-8")
    return f"Data saved to {path.as_posix()}"


save_tool = Tool(
    name="save_text_to_file",
    func=_save_to_txt,
    description=(
        """
        Purpose
        -------
        Persist any piece of plain text to a timestamped *.txt* file under ./output/.
        Ideal for jotting down idea drafts, SWOT analyses, user-stories, or scratch notes
        that you might want to share later.

        Parameters
        ----------
        data   : str   (required) â€“ the full body of text to save.
        prefix : str   (optional, default "research") â€“ filename prefix; the final name
                is <prefix>_YYYY-MM-DD_HH-MM-SS.txt.

        Returns
        -------
        Confirmation string with the relative file path.  No file object is returned.

        Caveats
        -------
        â€¢ Simply writes to disk â€“ it does NOT version existing files or push anything
        back to Jira/Drive.
        """
    ),
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RAG Retriever over Chroma â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHROMA_DIR = "rag_chroma_db"
_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
_retriever = Chroma(persist_directory=CHROMA_DIR, embedding_function=_embeddings).as_retriever(search_kwargs={"k": 4})


def _rag_lookup(query: str) -> str:
    docs: List[Document] = _retriever.invoke(query)
    if not docs:
        return "Å½Ã¡dnÃ© internÃ­ dokumenty se k dotazu nenaÅ¡ly."
    return "\n\n".join(d.page_content for d in docs)


rag_tool = Tool(
    name="rag_retriever",
    func=_rag_lookup,
    description=(
        """
        Purpose
        -------
        Semantic Retrieval-Augmented Generation over Productooâ€™s **internal knowledge**
        base (embedded in a Chroma DB).  Sources include P4 user documentation,
        roadmap pages, and archived conversation snippets.

        Parameters
        ----------
        query : str â€“ a natural-language question or keyword string.

        Returns
        -------
        Up to 4 highly relevant passages concatenated together.

        Typical use cases
        -----------------
        P4 application feature details, implementation scenarios, or roadmap justifications
        to ground an answer before calling an LLM.
        """
    ),
)

__all__ = ["save_tool", "rag_tool"]

```


---

### `tools\web_tools.py`

```python
from __future__ import annotations

import os
import openai
from langchain.tools import Tool
from langchain_community.tools import DuckDuckGoSearchRun, WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from tavily import TavilyClient

# Optâ€‘out from OpenAI telemetry
os.environ.setdefault("OPENAI_TELEMETRY", "0")
if hasattr(openai, "telemetry") and hasattr(openai.telemetry, "TelemetryClient"):
    openai.telemetry.TelemetryClient.capture = lambda *_, **__: None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ General Web Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_duck = DuckDuckGoSearchRun()
search_tool = Tool(
    name="searchWeb",
    func=_duck.run,
    description=(
        """
        Purpose
        -------
        Fast, general-purpose web lookup powered by DuckDuckGo.  Best for definitions,
        quick facts, fresh news articles, or blog posts.

        Parameters
        ----------
        query : str â€“ the search phrase.

        Returns
        -------
        DDGâ€™s textual snippet(s); no rich metadata or full RSS feed.

        When *not* to use
        -----------------
        For in-depth competitor or market research prefer tavily_search, which does
        semantic ranking.
        """
    ),
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Wikipedia Snippet â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=400)
wiki_tool = Tool(
    name="wikipedia_query",
    func=WikipediaQueryRun(api_wrapper=api_wrapper).run,
    description=(
        """
        Purpose
        -------
        Pull a concise (< 400 chars) summary paragraph from Wikipedia for quick
        background, historical context, industry standards, or terminology.

        Parameters
        ----------
        query : str â€“ topic name or phrase.

        Returns
        -------
        Plain-text summary (no infobox tables or references).
        """
    ),
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tavily Semantic Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_TAVILY_KEY = os.getenv("TAVILY_API_KEY")
_client: TavilyClient | None = None
if _TAVILY_KEY:
    _client = TavilyClient(api_key=_TAVILY_KEY)


def _tavily_search(query: str) -> str:
    if _client is None:
        return "Tavily nenÃ­ nakonfigurovÃ¡no (chybÃ­ TAVILY_API_KEY)."

    try:
        raw = _client.search(query=query, max_results=6)
    except Exception as exc:  # pragma: no cover - external call
        return f"Tavily search selhalo: {exc}"

    results = raw.get("results", [])
    if not results:
        return "Tavily nic nenaÅ¡lo."

    snippets = [f"- {r['url']}\n  {r['content'][:400].strip()}â€¦" for r in results]
    return "\n\n".join(snippets)


tavily_tool = Tool(
    name="tavily_search",
    func=_tavily_search,
    description=(
        """
        Purpose
        -------
        LLM-backed **semantic** web search via the Tavily API â€“ tailored for competitive
        intelligence, white-papers, press releases, or discovering market trends that
        keyword search might miss.

        Parameters
        ----------
        query : str â€“ question or topic.

        Returns
        -------
        Up to 6 items: each entry shows the URL plus a ~400-character snippet.

        Prerequisite
        ------------
        Environment variable TAVILY_API_KEY must be set; otherwise the tool politely
        reports it is unavailable.
        """
    ),
)

__all__ = ["search_tool", "wiki_tool", "tavily_tool"]

```


---

### `tools\__init__.py`

```python
# tools/__init__.py
"""Collection of LangChain tools split into dedicated modules."""
from __future__ import annotations

from .web_tools import search_tool, wiki_tool, tavily_tool
from .rag_tools import save_tool, rag_tool
from .jira_tools import (
    jira_ideas,
    jira_issue_detail,
    jira_duplicates,
    _JIRA,
    _jira_issue_detail,
    jira_update_description,
    jira_child_issues,
    jira_issue_links,
)
from .jira_content_tools import ALL_TOOLS as jira_content_tools
from .input_loader import process_input_tool

__all__ = [
    "search_tool",
    "wiki_tool",
    "rag_tool",
    "jira_ideas",
    "jira_issue_detail",
    "tavily_tool",
    "save_tool",
    "jira_duplicates",
    "_JIRA",
    "_jira_issue_detail",
    "jira_content_tools",
    "process_input_tool",
    "jira_update_description",
    "jira_child_issues",
    "jira_issue_links",
]

```


---

### `utils\io_utils.py`

```python

```
