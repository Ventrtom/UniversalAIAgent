# services/input_loader.py

"""
Input Loader ‚Äì import files from ./files do Chroma vektorov√© DB
---------------------------------------------------------------
‚û§ P≈ôetahuje soubory z lok√°ln√≠ slo≈æky `./files` (p≈ô√≠padnƒõ podmno≈æinu podle
  argumentu) a indexuje je pro RAG dotazy.

Argumenty (string):
    ""            ‚Üí pln√Ω sken (p≈ôeskoƒç√≠ u≈æ zaindexovan√© soubory)
    "force"       ‚Üí znovu zaindexuje v≈°echno (ignoruje fingerprinty)
    "pdf" / ".pdf"‚Üí jen danou p≈ô√≠ponu
    "subdir/foo"  ‚Üí jen dan√Ω (pod)adres√°≈ô
    "file.ext"    ‚Üí jen konkr√©tn√≠ soubor

Vrac√≠:
    Markdown report ‚Äì jedna ≈ô√°dka na ka≈æd√Ω novƒõ naimportovan√Ω soubor.
"""
from __future__ import annotations

import hashlib
import inspect
from pathlib import Path
import os
from typing import List

from langchain_chroma import Chroma
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores.utils import filter_complex_metadata

# ---- konfigurace -----------------------------------------------------------
CHROMA_DIR = os.getenv("CHROMA_DIR_V2", "data")
INPUT_DIR = Path("files")
INPUT_DIR.mkdir(exist_ok=True)

_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
_vectorstore = Chroma(
    collection_name="rag_store",
    embedding_function=_embeddings,
    persist_directory=CHROMA_DIR,
)

# ---- utilitky --------------------------------------------------------------
def _file_id(path: Path) -> str:
    """Fingerprint souboru = cesta::md5(content)."""
    h = hashlib.md5(path.read_bytes()).hexdigest()  # noqa: S324 ‚Äì lok√°ln√≠ hash
    return f"{path.as_posix()}::{h}"


def _already_indexed(file_id: str) -> bool:
    col = _vectorstore._collection
    return file_id in (m.get("file_id") for m in col.get(include=["metadatas"])["metadatas"])


def _summarise(text: str, llm: ChatOpenAI) -> str:
    prompt = "Shr≈à n√°sleduj√≠c√≠ dokument do max 1 vƒõty (ƒçe≈°tina):\n" + text[:4000]
    return llm.invoke(prompt).content.strip()


# ---- robustn√≠ sanitizace metadat ------------------------------------------
# API detekujeme dynamicky ‚Äì pokud sel≈æe vol√°n√≠ s Documentem, zkus√≠me dict
def _sanitize_metadata(doc: Document) -> None:
    """Zaruƒç√≠, ≈æe v metadata z≈Østanou jen skal√°ry a zavol√° korektn√≠ variantu
    filter_complex_metadata bez ohledu na verzi knihovny."""
    try:
        # üÖ∞Ô∏è novƒõj≈°√≠ API (bere Document, mutuje in-place)
        filter_complex_metadata(doc)
    except Exception:
        try:
            # üÖ±Ô∏è star≈°√≠ API (bere dict, vrac√≠ dict)
            doc.metadata = filter_complex_metadata(doc.metadata)  # type: ignore[arg-type]
        except Exception:
            # üÜò posledn√≠ z√°chrana ‚Äì ruƒçnƒõ p≈ôev√©st neskal√°rn√≠ hodnoty na str
            doc.metadata = {
                k: (str(v) if isinstance(v, (list, dict, set, tuple)) else v)
                for k, v in doc.metadata.items()
            }


# ----------------------------------------------------------------------------
def process_input_files(arg: str = "") -> str:
    """Hlavn√≠ API volan√© LangChain toolem."""
    arg = arg.strip()
    force_reindex = arg.lower() == "force"

    # -- vyhodnocen√≠ filtru ---------------------------------------------------
    ext_filter = None
    single_target: Path | None = None
    dir_filter: Path | None = None

    if arg and not force_reindex:
        if arg.lower().lstrip(".") in {"pdf", "docx", "pptx", "xlsx", "txt", "md", "html", "csv"}:
            ext_filter = arg.lower().lstrip(".")
        else:
            maybe_path = INPUT_DIR / arg
            if maybe_path.exists():
                single_target = maybe_path if maybe_path.is_file() else None
                dir_filter = maybe_path if maybe_path.is_dir() else None

    paths: List[Path] = (
        [single_target]
        if single_target
        else [p for p in (dir_filter or INPUT_DIR).rglob("*") if p.is_file()]
    )
    if ext_filter:
        paths = [p for p in paths if p.suffix.lower().lstrip(".") == ext_filter]
    if not paths:
        return "Nenalezeny ≈æ√°dn√© soubory odpov√≠daj√≠c√≠ zad√°n√≠."

    # -- naƒç√≠t√°n√≠ & chunkov√°n√≠ ------------------------------------------------
    from langchain_unstructured import UnstructuredLoader  # lazy import

    new_docs: List[Document] = []
    reports: List[str] = []
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

    for path in paths:
        fid = _file_id(path)
        if not force_reindex and _already_indexed(fid):
            continue

        # 1Ô∏è‚É£ naƒçti dokument(y)
        try:
            raw_docs = UnstructuredLoader(str(path)).load()
        except Exception as exc:  # pragma: no cover
            reports.append(f"- {path.name}: ‚ùå Nepoda≈ôilo se naƒç√≠st ({exc})")
            continue

        # 2Ô∏è‚É£ normalizuj na Document
        docs: List[Document] = []
        for item in raw_docs:
            if isinstance(item, Document):
                docs.append(item)
            elif isinstance(item, tuple) and len(item) == 2:
                content, meta = item
                docs.append(Document(page_content=str(content), metadata=dict(meta or {})))
            else:
                docs.append(Document(page_content=str(item), metadata={}))

        # 3Ô∏è‚É£ metadata & sanitizace
        base_meta = {"source": "input_folder", "file_path": str(path), "file_id": fid}
        for d in docs:
            d.metadata.update(base_meta)
            _sanitize_metadata(d)

        # 4Ô∏è‚É£ split + p≈ôidej do sb√≠rky
        new_docs.extend(splitter.split_documents(docs))

        # 5Ô∏è‚É£ shrnut√≠ pro report
        summary = _summarise(" ".join(d.page_content for d in docs)[:8000], llm)
        reports.append(f"- {path.name}: {summary}")

    if new_docs:
        _vectorstore.add_documents(new_docs)  # Chroma se ulo≈æ√≠ automaticky

    if not reports:
        return "≈Ω√°dn√© nov√© soubory nebyly naimportov√°ny."
    return f"‚úÖ Naimportov√°no {len(reports)} soubor≈Ø:\n" + "\n".join(reports)
